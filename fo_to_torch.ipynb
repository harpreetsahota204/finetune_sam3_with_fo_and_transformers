{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f279e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c87c3a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running the oldest supported major version of MongoDB. Please refer to https://deprecation.voxel51.com for deprecation notices. You can suppress this exception by setting your `database_validation` config parameter to `False`. See https://docs.voxel51.com/user_guide/config.html#configuring-a-mongodb-connection for more information\n"
     ]
    }
   ],
   "source": [
    "# # already have this dataset locally\n",
    "# import fiftyone as fo\n",
    "\n",
    "# dataset = fo.load_dataset(\"ris-lad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\"Voxel51/RIS-LAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af836d",
   "metadata": {},
   "source": [
    "## SAM Fine-tuning Dataset from FiftyOne\n",
    "\n",
    "This module converts a FiftyOne dataset with Detection masks into a PyTorch \n",
    "dataset compatible with SAM fine-tuning.\n",
    "\n",
    "The key challenge: FiftyOne stores masks efficiently by cropping them to the \n",
    "bounding box region. SAM expects full-image masks. We handle that conversion here.\n",
    "\n",
    "The pipeline follows FiftyOne's recommended pattern:\n",
    "1. Define a `GetItem` subclass to specify which fields to extract\n",
    "2. Use `dataset.to_torch()` to create an intermediate torch dataset  \n",
    "3. Wrap/flatten to handle multiple detections per sample\n",
    "\n",
    "\n",
    "### Step 1: Define a GetItem subclass\n",
    "\n",
    "- `GetItem` tells FiftyOne which fields to pull from each sample and how to  transform them. This is FiftyOne's bridge to PyTorch.\n",
    "\n",
    "- `required_keys`: list of field names to extract from each sample\n",
    "\n",
    "- `__call__`: transforms the extracted fields into your desired format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f780e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from fiftyone.utils.torch import GetItem\n",
    "\n",
    "class SAMDataGetter(GetItem):\n",
    "    \"\"\"\n",
    "    Extracts detection data from FiftyOne samples for SAM training.\n",
    "    \n",
    "    For each sample, pulls out:\n",
    "    - filepath: path to the image\n",
    "    - detections: the Detection objects containing bboxes and masks\n",
    "    - metadata: image dimensions (needed for coordinate conversion)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, detection_field=\"ground_truth\"):\n",
    "        self.detection_field = detection_field\n",
    "        # Must call super().__init__() - it sets up internal field mapping\n",
    "        super().__init__()\n",
    "    \n",
    "    @property\n",
    "    def required_keys(self):\n",
    "        # These are the FiftyOne sample fields we need access to\n",
    "        return [\"filepath\", self.detection_field, \"metadata\"]\n",
    "    \n",
    "    def __call__(self, d):\n",
    "        \"\"\"\n",
    "        Transform a FiftyOne sample dict into our intermediate format.\n",
    "        \n",
    "        We extract each detection as a separate item since SAM trains\n",
    "        on individual object masks, not full images with multiple objects.\n",
    "        \"\"\"\n",
    "        detections = d.get(self.detection_field)\n",
    "        metadata = d.get(\"metadata\")\n",
    "        \n",
    "        items = []\n",
    "        if detections is not None and hasattr(detections, 'detections'):\n",
    "            for det in detections.detections:\n",
    "                # Skip detections without masks\n",
    "                if det.mask is None:\n",
    "                    continue\n",
    "                    \n",
    "                items.append({\n",
    "                    \"filepath\": d.get(\"filepath\"),\n",
    "                    \"bounding_box\": det.bounding_box,  # [x, y, w, h] relative coords\n",
    "                    \"mask\": det.mask,                   # Cropped to bbox region\n",
    "                    \"label\": det.label,\n",
    "                    \"width\": metadata.width if metadata else None,\n",
    "                    \"height\": metadata.height if metadata else None,\n",
    "                    \"referring_expression\": getattr(det, 'referring_expression', None),\n",
    "                })\n",
    "        \n",
    "        return {\"items\": items}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74a82b",
   "metadata": {},
   "source": [
    "### Step 2: Flatten and process for SAM\n",
    "\n",
    "The `GetItem` above returns multiple detections per sample. We need to flatten this so each detection becomes its own training example.\n",
    "\n",
    "We also handle the FiftyOne â†’ SAM format conversion:\n",
    "- FiftyOne bbox: `[x, y, width, height]` in relative `[0,1]` coordinates\n",
    "- SAM bbox: `[x_min, y_min, x_max, y_max]` in absolute pixel coordinates\n",
    "- FiftyOne mask: cropped to bounding box region\n",
    "- SAM mask: full image size with object in correct location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38742764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenedSAMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Flattens the FiftyOne torch dataset so each detection is a separate item,\n",
    "    then processes each item into the format SAM expects.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fo_torch_dataset, processor):\n",
    "        self.processor = processor\n",
    "        self.items = []\n",
    "        \n",
    "        # Flatten: one training sample per detection\n",
    "        for sample in fo_torch_dataset:\n",
    "            for item in sample[\"items\"]:\n",
    "                if item[\"width\"] and item[\"height\"]:\n",
    "                    self.items.append(item)\n",
    "        \n",
    "        print(f\"FlattenedSAMDataset created with {len(self.items)} items\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        w, h = item[\"width\"], item[\"height\"]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(item[\"filepath\"]).convert(\"RGB\")\n",
    "        \n",
    "        # --- Bounding Box Conversion ---\n",
    "        # FiftyOne: [x, y, width, height] as fractions of image size (0 to 1)\n",
    "        # SAM: [x_min, y_min, x_max, y_max] in absolute pixels\n",
    "        rx, ry, rw, rh = item[\"bounding_box\"]\n",
    "        bbox = [\n",
    "            int(rx * w),           # x_min\n",
    "            int(ry * h),           # y_min\n",
    "            int((rx + rw) * w),    # x_max\n",
    "            int((ry + rh) * h)     # y_max\n",
    "        ]\n",
    "        \n",
    "        # --- Mask Conversion ---\n",
    "        # FiftyOne stores masks cropped to the bounding box to save space.\n",
    "        # SAM expects a full-image mask. We expand the cropped mask back\n",
    "        # to full size by placing it at the correct location.\n",
    "        mask = item[\"mask\"]\n",
    "        full_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        mask_h, mask_w = mask.shape\n",
    "        x_start, y_start = bbox[0], bbox[1]\n",
    "        full_mask[y_start:y_start + mask_h, x_start:x_start + mask_w] = mask\n",
    "        \n",
    "        # --- SAM Processor ---\n",
    "        # The processor handles image resizing, normalization, and \n",
    "        # preparing the bbox prompt in the format SAM expects\n",
    "        inputs = self.processor(image, input_boxes=[[bbox]], return_tensors=\"pt\")\n",
    "        \n",
    "        # Remove batch dimension added by processor (we batch later)\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        \n",
    "        # Add our ground truth mask for loss computation\n",
    "        inputs[\"ground_truth_mask\"] = full_mask\n",
    "        \n",
    "        # Add referring expression for text-prompted segmentation\n",
    "        inputs[\"referring_expression\"] = item[\"referring_expression\"]\n",
    "\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e0863",
   "metadata": {},
   "source": [
    "### Step 3: Collate function for DataLoader\n",
    "\n",
    "When batching samples, we need to handle the ground truth masks specially since they might have different sizes (if images have different dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c1765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for SAM training.\n",
    "    \n",
    "    Stacks tensors from the processor and handles ground truth masks\n",
    "    which may vary in size across samples.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Keys that need special handling\n",
    "    special_keys = {\"ground_truth_mask\", \"referring_expression\"}\n",
    "    \n",
    "    # Get all keys except special ones\n",
    "    keys = [k for k in batch[0].keys() if k not in special_keys]\n",
    "    \n",
    "    # Stack processor outputs\n",
    "    for key in keys:\n",
    "        values = [item[key] for item in batch]\n",
    "        if isinstance(values[0], torch.Tensor):\n",
    "            result[key] = torch.stack(values)\n",
    "        else:\n",
    "            result[key] = values\n",
    "    \n",
    "    # Ground truth masks: stack if same size, keep as list otherwise\n",
    "    ground_truth_masks = [\n",
    "        torch.tensor(item[\"ground_truth_mask\"], dtype=torch.float32) \n",
    "        for item in batch\n",
    "    ]\n",
    "    try:\n",
    "        result[\"ground_truth_mask\"] = torch.stack(ground_truth_masks)\n",
    "    except RuntimeError:\n",
    "        # Variable sizes - training loop will need to handle this\n",
    "        result[\"ground_truth_mask\"] = ground_truth_masks\n",
    "    \n",
    "    # Referring expressions: keep as list of strings\n",
    "    result[\"referring_expression\"] = [item[\"referring_expression\"] for item in batch]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693b24f",
   "metadata": {},
   "source": [
    "### Putting it all together: `create_dataloaders`\n",
    "\n",
    "This function orchestrates the full pipeline:\n",
    "\n",
    "1. Filter dataset by train/val tags\n",
    "\n",
    "2. Create `GetItem` and call `to_torch()`\n",
    "\n",
    "3. Wrap in `FlattenedSAMDataset`\n",
    "\n",
    "4. Create `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8249b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    fo_dataset,\n",
    "    processor,\n",
    "    batch_size=2,\n",
    "    detection_field=\"ground_truth\",\n",
    "    num_workers=0,\n",
    "    train_tag=\"train\",\n",
    "    val_tag=\"val\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create train and validation DataLoaders from a FiftyOne dataset.\n",
    "    \n",
    "    Args:\n",
    "        fo_dataset: FiftyOne dataset with Detection masks\n",
    "        processor: SamProcessor instance\n",
    "        batch_size: Batch size for training\n",
    "        detection_field: Name of the Detections field\n",
    "        num_workers: DataLoader workers\n",
    "        train_tag: Tag identifying training samples\n",
    "        val_tag: Tag identifying validation samples\n",
    "    \n",
    "    Returns:\n",
    "        train_dataloader, val_dataloader\n",
    "    \"\"\"\n",
    "    # Filter to train/val using existing tags on samples\n",
    "    train_view = fo_dataset.match_tags(train_tag)\n",
    "    val_view = fo_dataset.match_tags(val_tag)\n",
    "    \n",
    "    print(f\"Using existing tags: {len(train_view)} train samples, {len(val_view)} val samples\")\n",
    "    \n",
    "    # Step 1 & 2: Use GetItem pattern with to_torch()\n",
    "    # This creates a torch-compatible dataset that lazily loads FiftyOne data\n",
    "    data_getter = SAMDataGetter(detection_field=detection_field)\n",
    "    train_torch_dataset = train_view.to_torch(data_getter)\n",
    "    val_torch_dataset = val_view.to_torch(data_getter)\n",
    "    \n",
    "    print(f\"Intermediate datasets: train={len(train_torch_dataset)}, val={len(val_torch_dataset)}\")\n",
    "    \n",
    "    # Step 3: Flatten (one item per detection) and process for SAM\n",
    "    train_dataset = FlattenedSAMDataset(train_torch_dataset, processor)\n",
    "    val_dataset = FlattenedSAMDataset(val_torch_dataset, processor)\n",
    "    \n",
    "    # Create DataLoaders with our custom collate function\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b00ab26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.sam3 import Sam3Model, Sam3Processor\n",
    "\n",
    "processor = Sam3Processor.from_pretrained(\"facebook/sam3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "757b4bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing tags: 1682 train samples, 421 val samples\n",
      "Intermediate datasets: train=1682, val=421\n",
      "FlattenedSAMDataset created with 11156 items\n",
      "FlattenedSAMDataset created with 2715 items\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = create_dataloaders(\n",
    "    dataset,\n",
    "    processor,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f736200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values: torch.Size([2, 3, 1008, 1008])\n",
      "original_sizes: torch.Size([2, 2])\n",
      "input_ids: torch.Size([2, 32])\n",
      "attention_mask: torch.Size([2, 32])\n",
      "input_boxes: torch.Size([2, 1, 4])\n",
      "ground_truth_mask: torch.Size([2, 1080, 1080])\n",
      "referring_expression: list of 2 tensors\n"
     ]
    }
   ],
   "source": [
    "# Verify a batch\n",
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    else:\n",
    "        print(f\"{k}: list of {len(v)} tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016397d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
