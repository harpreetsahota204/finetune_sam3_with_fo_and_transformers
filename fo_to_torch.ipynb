{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f279e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c3a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running the oldest supported major version of MongoDB. Please refer to https://deprecation.voxel51.com for deprecation notices. You can suppress this exception by setting your `database_validation` config parameter to `False`. See https://docs.voxel51.com/user_guide/config.html#configuring-a-mongodb-connection for more information\n"
     ]
    }
   ],
   "source": [
    "# # already have this dataset locally\n",
    "# import fiftyone as fo\n",
    "\n",
    "# dataset = fo.load_dataset(\"ris-lad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\"Voxel51/RIS-LAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af836d",
   "metadata": {},
   "source": [
    "## SAM Fine-tuning Dataset from FiftyOne\n",
    "\n",
    "This module converts a FiftyOne dataset with Detection masks into a PyTorch \n",
    "dataset compatible with SAM fine-tuning.\n",
    "\n",
    "Key insight: FiftyOne's `to_patches()` method creates a view where each \n",
    "detection becomes its own sample. This eliminates the need to manually \n",
    "flatten detections - FiftyOne handles it for us.\n",
    "\n",
    "The pipeline:\n",
    "1. Convert dataset to patches view (one sample per detection)\n",
    "2. Define a `GetItem` to extract and transform each patch\n",
    "3. Use `to_torch()` to create the PyTorch dataset\n",
    "\n",
    "\n",
    "### Step 1: Define a GetItem subclass\n",
    "\n",
    "FiftyOne's `GetItem` class is the bridge between FiftyOne and PyTorch. It tells FiftyOne:\n",
    " 1. What fields to extract from each sample (via required_keys)\n",
    " 2. How to transform them into your desired format (via `__call__`)\n",
    "\n",
    "The `field_mapping` parameter is important when working with patches. In a  patches view, the detection data lives in the original field name (e.g., `ground_truth), but we want to access it with a generic name in our code.\n",
    "\n",
    "`field_mapping={\"detection\": \"ground_truth\"}` means:\n",
    " - In our code, we write `d.get(\"detection\")` \n",
    " - FiftyOne knows to pull from the `ground_truth` field\n",
    "\n",
    "This makes our `GetItem` reusable across datasets with different field names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from fiftyone.utils.torch import GetItem\n",
    "\n",
    "\n",
    "class SAMPatchGetItem(GetItem):\n",
    "    \"\"\"\n",
    "    Extracts and transforms patch data for SAM training.\n",
    "    \n",
    "    Each patch sample contains:\n",
    "    - filepath: path to the full image\n",
    "    - detection: the Detection object (bbox, mask, label, etc.)\n",
    "    - metadata: image dimensions\n",
    "    \n",
    "    We transform this into SAM's expected format:\n",
    "    - pixel_values: processed image tensor\n",
    "    - input_boxes: bbox in absolute pixel coords\n",
    "    - ground_truth_mask: full-image binary mask\n",
    "    - referring_expression: text prompt for the object\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processor, field_mapping=None):\n",
    "        self.processor = processor\n",
    "        # Must call super().__init__() with field_mapping - this sets up\n",
    "        # the internal mapping that FiftyOne uses to pull the right fields\n",
    "        super().__init__(field_mapping=field_mapping)\n",
    "    \n",
    "    @property\n",
    "    def required_keys(self):\n",
    "        # These are the keys we'll access in __call__.\n",
    "        # 'detection' is a virtual key that gets mapped to the real field\n",
    "        # via field_mapping. 'filepath' and 'metadata' are standard fields\n",
    "        # that exist on all FiftyOne samples.\n",
    "        return ['filepath', 'detection', 'metadata']\n",
    "    \n",
    "    def __call__(self, d):\n",
    "        \"\"\"\n",
    "        Transform a FiftyOne sample dict into SAM training format.\n",
    "        \n",
    "        This is where the FiftyOne → SAM conversion happens:\n",
    "        - Relative bbox coords → absolute pixel coords\n",
    "        - Cropped mask → full-image mask\n",
    "        - Raw image → processed tensor\n",
    "        \"\"\"\n",
    "        # Load full image (patches still reference the original image file)\n",
    "        image = Image.open(d[\"filepath\"]).convert(\"RGB\")\n",
    "        detection = d.get(\"detection\")\n",
    "        metadata = d.get(\"metadata\")\n",
    "        \n",
    "        w = metadata.width\n",
    "        h = metadata.height\n",
    "        \n",
    "        # --- Bounding Box Conversion ---\n",
    "        # FiftyOne stores bboxes as [x, y, width, height] with values in [0, 1]\n",
    "        # representing fractions of the image dimensions. This is great for\n",
    "        # resolution-independent storage, but SAM needs absolute pixel coords\n",
    "        # in [x_min, y_min, x_max, y_max] format.\n",
    "        rx, ry, rw, rh = detection.bounding_box\n",
    "        bbox = [\n",
    "            int(rx * w),           # x_min in pixels\n",
    "            int(ry * h),           # y_min in pixels\n",
    "            int((rx + rw) * w),    # x_max in pixels\n",
    "            int((ry + rh) * h)     # y_max in pixels\n",
    "        ]\n",
    "        \n",
    "        # --- Mask Conversion ---\n",
    "        # FiftyOne stores instance masks efficiently by cropping them to the\n",
    "        # bounding box region. A mask for a 50x50 pixel object is stored as a\n",
    "        # 50x50 array, not a full 1080x1080 array. This saves significant space.\n",
    "        #\n",
    "        # SAM expects full-image masks where the object pixels are in their\n",
    "        # actual location. We \"unpack\" the cropped mask by placing it into\n",
    "        # a full-size zero array at the correct position.\n",
    "        mask = detection.mask\n",
    "        full_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        if mask is not None:\n",
    "            mask_h, mask_w = mask.shape\n",
    "            x_start, y_start = bbox[0], bbox[1]\n",
    "            full_mask[y_start:y_start + mask_h, x_start:x_start + mask_w] = mask\n",
    "        \n",
    "        # --- SAM Processor ---\n",
    "        # The HuggingFace SamProcessor handles all the image preprocessing:\n",
    "        # - Resizes image to SAM's expected input size\n",
    "        # - Normalizes pixel values\n",
    "        # - Formats the bbox prompt correctly\n",
    "        # We pass input_boxes as a nested list [[bbox]] because SAM supports\n",
    "        # multiple prompts per image (we just have one).\n",
    "        inputs = self.processor(image, input_boxes=[[bbox]], return_tensors=\"pt\")\n",
    "        \n",
    "        # The processor adds a batch dimension (for single-image inference).\n",
    "        # We remove it here since DataLoader will batch multiple samples later.\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        \n",
    "        # Add our ground truth mask for loss computation during training\n",
    "        inputs[\"ground_truth_mask\"] = full_mask\n",
    "        \n",
    "        # Add referring expression for text-prompted segmentation (e.g., SAM3)\n",
    "        # getattr with default handles cases where the field doesn't exist\n",
    "        inputs[\"referring_expression\"] = getattr(detection, 'referring_expression', None)\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e0863",
   "metadata": {},
   "source": [
    "### Step 2: Collate function for DataLoader\n",
    "\n",
    "\n",
    "When PyTorch's DataLoader batches samples together, it needs to know how to combine them. The default collate works for simple tensors, but we have:\n",
    " - `ground_truth_mask`: might be different sizes if images have different dims\n",
    " - `referring_expression`: strings, not tensors\n",
    "\n",
    "A custom collate function tells DataLoader exactly how to handle each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for SAM training.\n",
    "    \n",
    "    Handles the quirks of our data:\n",
    "    - Stacks tensor outputs from the processor\n",
    "    - Handles variable-size ground truth masks\n",
    "    - Keeps referring expressions as a list of strings\n",
    "    \n",
    "    Args:\n",
    "        batch: List of dicts from SAMPatchGetItem.__call__\n",
    "        \n",
    "    Returns:\n",
    "        Dict with batched tensors and lists\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # These fields need special handling (not simple tensor stacking)\n",
    "    special_keys = {\"ground_truth_mask\", \"referring_expression\"}\n",
    "    \n",
    "    # Stack all the processor outputs (pixel_values, input_boxes, etc.)\n",
    "    keys = [k for k in batch[0].keys() if k not in special_keys]\n",
    "    for key in keys:\n",
    "        values = [item[key] for item in batch]\n",
    "        if isinstance(values[0], torch.Tensor):\n",
    "            result[key] = torch.stack(values)\n",
    "        else:\n",
    "            result[key] = values\n",
    "    \n",
    "    # Ground truth masks: try to stack (works if all same size)\n",
    "    # If images have different dimensions, keep as list and handle in training\n",
    "    ground_truth_masks = [\n",
    "        torch.tensor(item[\"ground_truth_mask\"], dtype=torch.float32) \n",
    "        for item in batch\n",
    "    ]\n",
    "    try:\n",
    "        result[\"ground_truth_mask\"] = torch.stack(ground_truth_masks)\n",
    "    except RuntimeError:\n",
    "        # Different sizes - training loop needs to handle per-sample\n",
    "        result[\"ground_truth_mask\"] = ground_truth_masks\n",
    "    \n",
    "    # Referring expressions are strings - just keep as list\n",
    "    result[\"referring_expression\"] = [item[\"referring_expression\"] for item in batch]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693b24f",
   "metadata": {},
   "source": [
    "### Putting it all together: `create_dataloaders`\n",
    "\n",
    "This function ties everything together. The key FiftyOne concepts:\n",
    "\n",
    "1. Views: A \"view\" is a filtered/transformed lens on your dataset. The\n",
    "    underlying data doesn't change - you're just looking at it differently.\n",
    "    match_tags(\"train\") gives you a view of only training samples.\n",
    "\n",
    " 2. Patches: `to_patches(field)` creates a view where each detection in that\n",
    "    field becomes its own sample. If you have 100 images with 5 detections\n",
    "    each, `to_patches` gives you 500 patch samples. This is perfect for \n",
    "    instance-level training like SAM.\n",
    "\n",
    " 3. `to_torch()`: Converts a FiftyOne view to a PyTorch Dataset using your\n",
    "    GetItem class to define how each sample is loaded and transformed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8249b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    fo_dataset,\n",
    "    processor,\n",
    "    batch_size=2,\n",
    "    detection_field=\"ground_truth\",\n",
    "    num_workers=0,\n",
    "    train_tag=\"train\",\n",
    "    val_tag=\"val\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create train and validation DataLoaders from a FiftyOne dataset.\n",
    "    \n",
    "    Uses FiftyOne's patches view to automatically flatten detections,\n",
    "    then converts to PyTorch format.\n",
    "    \n",
    "    Args:\n",
    "        fo_dataset: FiftyOne dataset with Detection masks\n",
    "        processor: SamProcessor instance\n",
    "        batch_size: Batch size for training\n",
    "        detection_field: Name of the Detections field containing your masks\n",
    "        num_workers: DataLoader workers (0 = main process only)\n",
    "        train_tag: Tag identifying training samples\n",
    "        val_tag: Tag identifying validation samples\n",
    "    \n",
    "    Returns:\n",
    "        train_dataloader, val_dataloader: Ready for training loop\n",
    "    \"\"\"\n",
    "    # --- Step 1: Filter by split tags ---\n",
    "    # match_tags returns a view containing only samples with that tag.\n",
    "    # Your dataset should already have \"train\"/\"val\" tags on each sample.\n",
    "    train_view = fo_dataset.match_tags(train_tag)\n",
    "    val_view = fo_dataset.match_tags(val_tag)\n",
    "    \n",
    "    print(f\"Samples - train: {len(train_view)}, val: {len(val_view)}\")\n",
    "    \n",
    "    # --- Step 2: Convert to patches ---\n",
    "    # This is the key step that makes everything cleaner. to_patches() creates\n",
    "    # a view where each detection becomes its own sample. \n",
    "    #\n",
    "    # Before: 1 sample with 6 detections\n",
    "    # After:  6 patch samples, each with 1 detection\n",
    "    #\n",
    "    # This means we don't need custom flattening logic - FiftyOne handles it.\n",
    "    train_patches = train_view.to_patches(detection_field)\n",
    "    val_patches = val_view.to_patches(detection_field)\n",
    "    \n",
    "    print(f\"Patches - train: {len(train_patches)}, val: {len(val_patches)}\")\n",
    "    \n",
    "    # --- Step 3: Set up field mapping ---\n",
    "    # In the patches view, each sample's detection data lives in the original\n",
    "    # field (e.g., \"ground_truth\"). field_mapping lets us access it with a\n",
    "    # generic name in our GetItem code.\n",
    "    #\n",
    "    # This makes SAMPatchGetItem reusable - it always uses d.get(\"detection\"),\n",
    "    # and field_mapping tells FiftyOne which actual field that refers to.\n",
    "    field_mapping = {\"detection\": detection_field}\n",
    "    \n",
    "    train_getter = SAMPatchGetItem(processor, field_mapping=field_mapping)\n",
    "    val_getter = SAMPatchGetItem(processor, field_mapping=field_mapping)\n",
    "    \n",
    "    # --- Step 4: Convert to PyTorch datasets ---\n",
    "    # to_torch() wraps the FiftyOne view in a PyTorch Dataset interface.\n",
    "    # When you access dataset[i], it calls your GetItem to load and transform\n",
    "    # that sample on-the-fly.\n",
    "    train_dataset = train_patches.to_torch(train_getter)\n",
    "    val_dataset = val_patches.to_torch(val_getter)\n",
    "    \n",
    "    # --- Step 5: Create DataLoaders ---\n",
    "    # Standard PyTorch DataLoaders with our custom collate function.\n",
    "    # shuffle=True for training to randomize batch composition each epoch.\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b00ab26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.sam3 import Sam3Model, Sam3Processor\n",
    "\n",
    "processor = Sam3Processor.from_pretrained(\"facebook/sam3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "757b4bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples - train: 1682, val: 421\n",
      "Patches - train: 11156, val: 2715\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = create_dataloaders(\n",
    "    dataset,\n",
    "    processor,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f736200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values: torch.Size([2, 3, 1008, 1008])\n",
      "original_sizes: torch.Size([2, 2])\n",
      "input_ids: torch.Size([2, 32])\n",
      "attention_mask: torch.Size([2, 32])\n",
      "input_boxes: torch.Size([2, 1, 4])\n",
      "ground_truth_mask: torch.Size([2, 1080, 1080])\n",
      "referring_expression: list of 2 tensors\n"
     ]
    }
   ],
   "source": [
    "# Verify a batch\n",
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    else:\n",
    "        print(f\"{k}: list of {len(v)} items\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
