{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone umap-learn\n",
    "!pip install git+https://github.com/huggingface/transformers.git#egg=transformers\n",
    "!pip install shapely\n",
    "!pip install trackio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c11245",
   "metadata": {},
   "source": [
    "In this tutorial we'll make use of the [RIS-LAD](https://huggingface.co/datasets/Voxel51/RIS-LAD) dataset. [RIS-LAD is the first fine-grained benchmark](https://arxiv.org/abs/2507.20920) designed specifically for low-altitude drone image segmentation.\n",
    "\n",
    "The dataset features 13,871 annotations with image-text-mask triplets captured from real drone footage at 30-100 meter altitudes with oblique viewing angles. Unlike existing remote sensing datasets that rely on high-altitude satellite imagery, RIS-LAD focuses on the visual complexities of low-altitude drone perception. These challenges include perspective changes, densely packed tiny objects, variable lighting conditions, and the notorious problems of **category drift** (tiny targets causing confusion with larger, semantically similar objects) and **object drift** (difficulty distinguishing among crowded same-class instances) that plague crowded aerial scenes.\n",
    "\n",
    "This benchmark addresses the gap in understanding how Visual AI systems see the world from a drone's perspective.\n",
    "\n",
    "You can download the dataset from the Hugging Face Hub as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c562b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/RIS-LAD\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863f43c",
   "metadata": {},
   "source": [
    "This dataset is in [FiftyOne format](https://docs.voxel51.com/user_guide/using_datasets.html). \n",
    "\n",
    "FiftyOne provides powerful functionality to inspect, search, and modify it from a [Dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset)-wide down to a [Sample](https://docs.voxel51.com/api/fiftyone.utils.data.html#fiftyone.utils.data.Sample) level.\n",
    "\n",
    "To see the schema of this dataset, you can simply call the Dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbaefc",
   "metadata": {},
   "source": [
    "A FiftyOne dataset is comprised of [Samples](https://docs.voxel51.com/api/fiftyone.utils.data.html#fiftyone.utils.data.Sample).  \n",
    "\n",
    "Samples store all information associated with a particular piece of data in a dataset, including basic metadata about the data, one or more sets of labels, and additional features associated with subsets of the data and/or label sets.\n",
    "\n",
    "The attributes of a Sample are called [Fields](https://docs.voxel51.com/api/fiftyone.core.fields.html#fiftyone.core.fields.Field), which stores information about the Sample. When a new Field is assigned to a Sample in a Dataset, it is automatically added to the dataset’s schema and thus accessible on all other samples in the dataset.\n",
    "\n",
    "To see the schema of a single Sample and the contents of its Fields, you can call the [`first()` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10948180",
   "metadata": {},
   "source": [
    "You can use the FiftyOne SDK to quickly compute some high-level statistics about your dataset with it's [built-in Aggregration methods](https://docs.voxel51.com/user_guide/using_aggregations.html).\n",
    "\n",
    "For example, you can use the [`count()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count) to compute the number of non-None field values in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f28833",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988745c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count(\"ground_truth.detections.referring_expression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd2b0f",
   "metadata": {},
   "source": [
    "You can use the [`count_values()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count_values) to compute the occurrences of field values in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446256d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32170629",
   "metadata": {},
   "source": [
    "You can use the [`distinct()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.distinct) to compute the distinct values of a field in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b97f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.distinct(\"ground_truth.detections.referring_expression\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342c546",
   "metadata": {},
   "source": [
    "### Adding a new Field to the Dataset\n",
    "\n",
    "A useful piece of information to have about a sample is the number of detection labels in that sample.  You can easily add this to each sample in your Dataset using a `ViewField` expression.  \n",
    "\n",
    "[`ViewField`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewField) and [`ViewExpression`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewExpression) classes allow you to use native Python operators to define expression. Simply wrap the target field of your sample in a `ViewField` and then apply comparison, logic, arithmetic or array operations to it to create a `ViewExpression`\n",
    "\n",
    "The idiomatic FiftyOne way to count the number of instance labels in a sample is to use a `ViewField` expression to access the list of labels and then use `.length()` to count them.\n",
    "\n",
    "To add the number of instances per image as a field on each sample in your dataset, you can use FiftyOne's [`set_values()`](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.set_values) method. This will efficiently compute and store the count for each sample.\n",
    "\n",
    "You can learn more about creating Dataset Views [in these docs](https://docs.voxel51.com/user_guide/using_views.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "num_instances = dataset.values(F(\"ground_truth.detections\").length())\n",
    "\n",
    "dataset.set_values(\"num_instances\", num_instances)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa8df5",
   "metadata": {},
   "source": [
    "In a similar manner, you can count the number of unique instance types for each sample in your Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "labels_per_sample = dataset.values(\"ground_truth.detections.label\")\n",
    "\n",
    "num_distinct_labels_per_sample = [len(set(labels)) if labels else 0 for labels in labels_per_sample]\n",
    "\n",
    "dataset.set_values(\"num_unique_instances\", num_distinct_labels_per_sample)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19f7cd",
   "metadata": {},
   "source": [
    "You can then combine these values together to create a complexity score for each Sample in your Dataset. As a simple example you can define the complexity score as number of instances + number of unique instance types. Note that the [`.values()` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.values) is used for efficiently extracting a slice of field across all Samples in a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_instance_counts = dataset.values(\"num_unique_instances\")\n",
    "\n",
    "num_instances_values = dataset.values(\"num_instances\")\n",
    "\n",
    "# Compute complexity scores for all samples\n",
    "complexity_scores = [nd + nul for nd, nul in zip(num_instances_values, unique_instance_counts)]\n",
    "\n",
    "# Set the values\n",
    "dataset.set_values(\"complexity_score\", complexity_scores)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f14533",
   "metadata": {},
   "source": [
    "There's a lot of interesting and non-trival things, like those shown above, that you can do with Fiftyone. Here are some additional resources for you to check out later:\n",
    "\n",
    "- For those familar with `pandas` you may want to check out this [pandas v FiftyOne cheat sheet](https://docs.voxel51.com/cheat_sheets/pandas_vs_fiftyone.html) to learn how to you can translate common pandas operations into FiftyOne syntax. \n",
    "\n",
    "- How to [create Views of your Dataset](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html) \n",
    "\n",
    "- [Filtering cheat sheet docs](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html)\n",
    "\n",
    "Of course, the most interesting part of FiftyOne is [the FiftyOne App](https://docs.voxel51.com/user_guide/app.html#using-the-fiftyone-app) (which runs locally on your machine). Something that can help us in exploring our Dataset in the App is [the Dashboard plugin](https://docs.voxel51.com/plugins/plugins_ecosystem/dashboard.html). You can install the Plugin as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc0e90",
   "metadata": {},
   "source": [
    "FiftyOne is open-source and hackable, and it has a robust framework for [building Plugins](https://docs.voxel51.com/plugins/developing_plugins.html), which allow you to extend and customize the functionality of the core tool to suit your specific needs.  FiftyOne has integrations with various computer vision models and other popular AI tools, [browse this curated collection of plugins](https://docs.voxel51.com/plugins/) to see how you can transform FiftyOne into a bespoke visual AI development workbench.\n",
    "\n",
    "To launch the FiftyOne App, all you need to do is run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd6da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49871630",
   "metadata": {},
   "source": [
    "<img src=\"ris_lad_in_fo_1.gif\">\n",
    "\n",
    "\n",
    "Of course, you can go deeper in the analysis of your dataset by [visualizing image embeddings](https://docs.voxel51.com/brain.html#visualizing-embeddings) in the App. You can use one of the the models from the [FiftyOne Model Zoo](https://docs.voxel51.com/model_zoo/overview.html), or a custom model which you can integrate as a [Remote Zoo Model](https://docs.voxel51.com/model_zoo/remote.html#remotely-sourced-zoo-models).\n",
    "\n",
    "One example of a Remote Zoo Model is the integration of [SigLIP2](https://docs.voxel51.com/plugins/plugins_ecosystem/siglip2.html), which you can use to visualize image embeddings, perform zero shot classification, and perform image retrieval by [searching via natural language](https://docs.voxel51.com/brain.html#text-similarity) in the App.\n",
    "\n",
    "Let's start by registering the Remote Zoo Model source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register this custom model source\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/siglip2\", \n",
    "    overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2c1ea",
   "metadata": {},
   "source": [
    "Then instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "siglip_model = foz.load_zoo_model(\n",
    "    \"google/siglip2-giant-opt-patch16-256\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ad369",
   "metadata": {},
   "source": [
    "You can than use the [`compute_embeddings()` method](https://docs.voxel51.com/api/fiftyone.core.models.html#fiftyone.core.models.compute_embeddings) of the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80705398",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=siglip_model,\n",
    "    embeddings_field=\"siglip2_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd2ba2",
   "metadata": {},
   "source": [
    "Then use the [`compute_visualization()` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_visualization) to generate low-dimensional representations of the samples (and/or individual objects) in your Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"siglip2_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"siglip2_viz\",\n",
    "    num_dims=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3f6a6",
   "metadata": {},
   "source": [
    "You can then use the [`compute_similarity()` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_similarity) to build a similarity index over the images in your dataset, which allows you to sort by similarity or search with natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a similarity index\n",
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"google/siglip2-giant-opt-patch16-256\",\n",
    "    embeddings=\"siglip2_embeddings\",\n",
    "    brain_key=\"siglip2_similarity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e668e",
   "metadata": {},
   "source": [
    "With the embeddings computed you can perform a lot of non-trival math, such as computing scores for [uniqueness](https://docs.voxel51.com/brain.html#image-uniqueness), [representativeness](https://docs.voxel51.com/brain.html#image-representativeness), and [identifying near duplicates](https://docs.voxel51.com/brain.html#near-duplicates) with simple function calls. \n",
    "\n",
    "\n",
    "We can use the same SigLIP2 model to perform zero-shot classification and further enrich our Dataset with information it didn't have before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip_model.text_prompt = \"Low altitude drone footage taken at \"\n",
    "siglip_model.classes = [\"day\", \"night\", \"dusk\"]\n",
    "\n",
    "dataset.apply_model(\n",
    "    siglip_model,\n",
    "    label_field=\"time_of_day\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d60ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip_model.text_prompt = \"The scene in this low altitude drone footage is in a \"\n",
    "siglip_model.classes = [\"urban area\", \"near water\", \"highway\", \"pedestrian area\"]\n",
    "\n",
    "dataset.apply_model(\n",
    "    siglip_model,\n",
    "    label_field=\"location\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e12bbd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's launch the App again and see what we can uncover by inspecting [the Embeddings panel](https://docs.voxel51.com/user_guide/app.html#embeddings-panel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ac8dfb",
   "metadata": {},
   "source": [
    "<img src=\"ris_lad_in_fo_2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cde1a1",
   "metadata": {},
   "source": [
    "# Using SAM 3\n",
    "\n",
    "We can use [SAM 3 in FiftyOne](https://docs.voxel51.com/plugins/plugins_ecosystem/sam3_images.html) as a Remote Zoo Model. The pattern is exactly as we have seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register the remote model source\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/sam3_images\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "sam3_model = foz.load_zoo_model(\"facebook/sam3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d70d0",
   "metadata": {},
   "source": [
    "The implementation in Fiftyone also allows us to compute embeddings for images using SAM 3 as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "sam3_model.pooling_strategy = \"max\"  # or \"mean\", \"cls\"\n",
    "\n",
    "dataset.compute_embeddings(\n",
    "    sam3_model,\n",
    "    embeddings_field=\"sam_embeddings\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Visualize with UMAP\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    method=\"umap\",\n",
    "    brain_key=\"sam_viz\",\n",
    "    embeddings=\"sam_embeddings\",\n",
    "    num_dims=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87598072",
   "metadata": {},
   "source": [
    "To run the SAM 3 model on the dataset, all we have to do is set some values for the model, and use the [`apply_model()` of the Dataset](docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.dataset.apply_model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam3_model.operation = \"concept_segmentation\"\n",
    "sam3_model.threshold = 0.5\n",
    "sam3_model.mask_threshold = 0.5\n",
    "\n",
    "sam3_model.prompt = dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "dataset.apply_model(\n",
    "    sam3_model,\n",
    "    label_field=\"sam3_not_finetuned\",\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    skip_failures=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa2ecf",
   "metadata": {},
   "source": [
    "We can view the embeddings and the predictions in the App as well:\n",
    "\n",
    "<img src=\"ris_lad_in_fo_3.gif\">\n",
    "\n",
    "We can then use [FiftyOne's evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) to see how well the initial results. You can [`evaluate_detections()` method](https://docs.voxel51.com/user_guide/evaluation.html#detections) to evaluate the predictions of an object detection model stored in a [`Detections`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Detections), [`Polylines`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Polylines), or [`Keypoints`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Keypoints) field of your dataset or of a temporal detection model stored in a [`TemporalDetections`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.TemporalDetection) field of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd15f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_detections(\n",
    "    \"sam3_not_finetuned\",          # Detections with masks\n",
    "    gt_field=\"ground_truth\",   # Detections with masks\n",
    "    eval_key=\"initial_sam3_eval\",\n",
    "    use_masks=True,            # use instance masks for IoU\n",
    "    compute_mAP=True,\n",
    "    tolerance=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a48bb5",
   "metadata": {},
   "source": [
    "The `evaluate_detections()` method returns a [`DetectionResults` instance](https://docs.voxel51.com/api/fiftyone.utils.eval.detection.html#fiftyone.utils.eval.detection.DetectionResults) that provides a variety of methods for generating various aggregate evaluation reports about your model.\n",
    "\n",
    "In addition, when you specify an `eval_key` parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth objects that you can leverage via the FiftyOne App to interactively explore the strengths and weaknesses of your model on individual samples.\n",
    "\n",
    "You can print the report to get a high-level picture of the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff1b500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     bicycle       0.23      0.35      0.28       640\n",
      "        boat       0.32      0.79      0.45       245\n",
      "         bus       0.60      0.71      0.65       732\n",
      "         car       0.34      0.85      0.48      4365\n",
      "       motor       0.23      0.80      0.35      2803\n",
      "      people       0.20      0.50      0.29      2910\n",
      "    tricycle       0.54      0.43      0.48       528\n",
      "       truck       0.51      0.49      0.50      1648\n",
      "\n",
      "   micro avg       0.29      0.68      0.40     13871\n",
      "   macro avg       0.37      0.62      0.43     13871\n",
      "weighted avg       0.32      0.68      0.42     13871\n",
      "\n",
      "0.27819947196402245\n"
     ]
    }
   ],
   "source": [
    "results.print_report()\n",
    "print(results.mAP())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebafbd",
   "metadata": {},
   "source": [
    "You can also open the [Model Evaluation Panel](https://docs.voxel51.com/api/fiftyone.utils.eval.detection.html#fiftyone.utils.eval.detection.DetectionResults) to visualize and interactively explore the evaluation results in the App:\n",
    "\n",
    "<img src=\"ris_lad_in_fo_4.gif\">\n",
    "\n",
    "\n",
    "You can use [Scenario Analysis](https://docs.voxel51.com/user_guide/app.html#scenario-analysis-sub-new) for a deep dive into model behavior across different scenarios.\n",
    "\n",
    "This evaluation technique helps uncover edge cases, identify annotation errors, and understand performance variations in different contexts. It gives you a better insight into your model's strengths and weaknesses while enabling meaningful comparisons of performance under varying input conditions. \n",
    "\n",
    "Ultimately, this detailed analysis helps improve training data quality and builds intuition about when and why your model succeeds or fails.\n",
    "\n",
    "<img src=\"ris_lad_in_fo_5.gif\">\n",
    "\n",
    "#### We're almost ready to fine-tune the model, but before we do we should check if there is any data leakage between the train and validation sets of the dataset.\n",
    "\n",
    "Our dataset has [Sample level tags](https://docs.voxel51.com/user_guide/basics.html#tags) which indicate which split each sample belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6bd115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train', 'val']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.distinct(\"tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d2e20",
   "metadata": {},
   "source": [
    "Despite our best efforts, duplicates and other forms of non-IID samples show up in our data. \n",
    "\n",
    "When these samples end up in different splits, [this can have consequences when evaluating a model](https://voxel51.com/blog/on-leaky-datasets-and-a-clever-horse). It can often be easy to overestimate model capability due to this issue. The FiftyOne Brain offers a way to identify such cases in dataset splits.\n",
    "\n",
    "The leaks of a dataset can be computed directly without the need for the predictions of a pre-trained model via the [`compute_leaky_splits()`](https://docs.voxel51.com/brain.html#leaky-splits) method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e048ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        Voxel51/RIS-LAD\n",
       "Media type:  image\n",
       "Num samples: 2103\n",
       "Persistent:  True\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:                   fiftyone.core.fields.ObjectIdField\n",
       "    filepath:             fiftyone.core.fields.StringField\n",
       "    tags:                 fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:             fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:           fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at:     fiftyone.core.fields.DateTimeField\n",
       "    ground_truth:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    prompts:              fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    siglip2_embeddings:   fiftyone.core.fields.VectorField\n",
       "    time_of_day:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    location:             fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    sam_embeddings:       fiftyone.core.fields.VectorField\n",
       "    sam3_not_finetuned:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    initial_sam3_eval_tp: fiftyone.core.fields.IntField\n",
       "    initial_sam3_eval_fp: fiftyone.core.fields.IntField\n",
       "    initial_sam3_eval_fn: fiftyone.core.fields.IntField"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "split_tags = [\"train\", \"val\"]\n",
    "\n",
    "index = fob.compute_leaky_splits(\n",
    "    dataset, \n",
    "    splits=split_tags,\n",
    "    embeddings=\"sam_embeddings\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e42157",
   "metadata": {},
   "source": [
    "The [`leaks_view()` method](https://docs.voxel51.com/api/fiftyone.brain.internal.core.leaky_splits.html#fiftyone.brain.internal.core.leaky_splits.LeakySplitsIndex.leaks_view) returns a view that contains only the leaks in the input splits. Once you have these leaks, it is wise to look through them. You may gain some insight into the source of the leaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ee55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks = index.leaks_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f26d23",
   "metadata": {},
   "source": [
    "You can launch the app on this view like so:\n",
    "\n",
    "```python\n",
    "session = fo.launch_app(leaks)\n",
    "```\n",
    "\n",
    "Fortunately for us, there are no leaks between our splits. But, it's always a good idea to check.\n",
    "\n",
    "We're now ready to fine-tune SAM 3\n",
    "\n",
    "# SAM 3 Fine-tuning Dataset from FiftyOne\n",
    "\n",
    "First, we need to convert the FiftyOne Dataset to a torch Dataset.\n",
    "\n",
    "In this section we will convert a FiftyOne dataset with Detection masks into a PyTorch \n",
    "dataset compatible with SAM fine-tuning.\n",
    "\n",
    "Key insight: FiftyOne's [`to_patches()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_patches) method creates a view where each \n",
    "detection becomes its own sample. This eliminates the need to manually \n",
    "flatten detections - FiftyOne handles it for us.\n",
    "\n",
    "The pipeline:\n",
    "1. Define a [`GetItem`](https://docs.voxel51.com/api/fiftyone.utils.torch.html#fiftyone.utils.torch.GetItem) to extract and transform each patch\n",
    "\n",
    "2. Define a Collate Function\n",
    "\n",
    "3. Split and flatten the dataset by converting dataset to patches view (one sample per detection)\n",
    "\n",
    "4. Use [`to_torch()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_torch) to create the DataLoader PyTorch dataset\n",
    "\n",
    "\n",
    "### Step 1: Define a GetItem subclass\n",
    "\n",
    "FiftyOne's `GetItem` class is the bridge between FiftyOne and PyTorch. It tells \n",
    "FiftyOne:\n",
    "\n",
    " 1. What fields to extract from each sample (via `required_keys`)\n",
    " \n",
    " 2. How to transform them into your desired format (via `__call__`)\n",
    "\n",
    "The `field_mapping` parameter is important when working with patches. In a \n",
    "patches view, the detection data lives in the original field name (e.g., \n",
    "\"ground_truth\"), but we want to access it with a generic name in our code.\n",
    "\n",
    "`field_mapping={\"detection\": \"ground_truth\"}` means:\n",
    " - In our code, we write `d[\"detection\"]` \n",
    " - FiftyOne knows to pull from the \"ground_truth\" field\n",
    "\n",
    "This makes our `GetItem` reusable across datasets with different field names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b72c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fiftyone.utils.torch import GetItem\n",
    "\n",
    "class SAMPatchGetItem(GetItem):\n",
    "    \"\"\"\n",
    "    Extracts and transforms patch data for SAM training.\n",
    "    \n",
    "    Each patch sample contains:\n",
    "    - filepath: path to the full image\n",
    "    - detection: the Detection object (bbox, mask, label, etc.)\n",
    "    - metadata: image dimensions\n",
    "    \n",
    "    We transform this into SAM's expected format:\n",
    "    - pixel_values: processed image tensor\n",
    "    - mask_labels: ground truth mask resized to match model input\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processor, field_mapping=None):\n",
    "        self.processor = processor\n",
    "        # Must call super().__init__() with field_mapping - this sets up\n",
    "        # the internal mapping that FiftyOne uses to pull the right fields\n",
    "        super().__init__(field_mapping=field_mapping)\n",
    "\n",
    "    @property\n",
    "    def required_keys(self):\n",
    "        # These are the keys we'll access in __call__.\n",
    "        # 'detection' is a virtual key that gets mapped to the real field\n",
    "        # via field_mapping. 'filepath' and 'metadata' are standard fields\n",
    "        # that exist on all FiftyOne samples.\n",
    "        return [\"filepath\", \"detection\", \"metadata\"]\n",
    "\n",
    "    def __call__(self, d):\n",
    "        \"\"\"\n",
    "        Transform a FiftyOne sample dict into SAM training format.\n",
    "        \n",
    "        This is where the FiftyOne → SAM conversion happens:\n",
    "        - Cropped mask → full-image mask → resized to model input size\n",
    "        - Raw image + text label → processed tensors\n",
    "        \"\"\"\n",
    "        # Load full image (patches still reference the original image file)\n",
    "        image = Image.open(d[\"filepath\"]).convert(\"RGB\")\n",
    "        detection = d[\"detection\"]\n",
    "        metadata = d[\"metadata\"]\n",
    "\n",
    "        # Get image dimensions (cast to int for safety)\n",
    "        w = int(metadata.width)\n",
    "        h = int(metadata.height)\n",
    "\n",
    "        # --- Bounding Box Extraction ---\n",
    "        # FiftyOne stores bboxes as [x, y, width, height] with values in [0, 1].\n",
    "        # We only need the top-left corner (x0, y0) to position the mask.\n",
    "        rx, ry, rw, rh = detection.bounding_box\n",
    "        x0 = int(rx * w)  # Top-left x in pixels\n",
    "        y0 = int(ry * h)  # Top-left y in pixels\n",
    "\n",
    "        # --- Mask Conversion ---\n",
    "        # FiftyOne stores masks cropped to the bounding box to save space.\n",
    "        # We expand the cropped mask back to full image size by placing it\n",
    "        # at the correct position (x0, y0).\n",
    "        full_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        m = detection.mask\n",
    "        mh, mw = m.shape\n",
    "        full_mask[y0 : y0 + mh, x0 : x0 + mw] = m.astype(np.uint8)\n",
    "\n",
    "        # --- Text Prompt ---\n",
    "        # Use the detection's class label as the text prompt for SAM\n",
    "        text = detection[\"label\"]\n",
    "\n",
    "        # --- SAM Processor ---\n",
    "        # The processor handles image preprocessing and text tokenization.\n",
    "        # For text-prompted SAM variants, we pass both image and text.\n",
    "        inputs = self.processor(images=image, text=text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Remove batch dimension added by processor (we batch later in DataLoader)\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "        # --- Resize Mask to Match Model Input ---\n",
    "        # The processor resizes the image to the model's expected input size.\n",
    "        # We need to resize our ground truth mask to match, so the loss\n",
    "        # computation compares tensors of the same shape.\n",
    "        hm = int(inputs[\"pixel_values\"].shape[-2])  # Model input height\n",
    "        wm = int(inputs[\"pixel_values\"].shape[-1])  # Model input width\n",
    "        \n",
    "        # Convert mask to tensor and add batch+channel dims for interpolate\n",
    "        mask_t = torch.from_numpy(full_mask).float()[None, None, ...]  # (1, 1, H, W)\n",
    "        \n",
    "        # Resize using nearest neighbor to preserve binary mask values\n",
    "        mask_rs = F.interpolate(mask_t, size=(hm, wm), mode=\"nearest\").squeeze(0)  # (1, H, W)\n",
    "\n",
    "        # Add resized mask as the training label\n",
    "        inputs[\"mask_labels\"] = mask_rs\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82215597",
   "metadata": {},
   "source": [
    "### Step 2: Collate function for DataLoader\n",
    "\n",
    "When PyTorch's DataLoader batches samples together, it needs to know how to combine them. \n",
    "\n",
    "The default collate works for simple tensors, but we have:\n",
    "\n",
    " - `mask_labels`: must stay as a list because SAM3 expects variable-length \n",
    "   targets per image (even though we have one mask per sample here, the model\n",
    "   interface expects a list)\n",
    "\n",
    "A custom collate function tells DataLoader exactly how to handle each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b16d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for SAM3 training.\n",
    "\n",
    "    Handles batching of samples from SAMPatchGetItem:\n",
    "    - Stacks tensor fields (pixel_values, input_ids, attention_mask, etc.)\n",
    "    - Keeps mask_labels as a list of tensors (SAM3's expected format)\n",
    "    \n",
    "    Args:\n",
    "        batch: List of dicts from SAMPatchGetItem.__call__\n",
    "        \n",
    "    Returns:\n",
    "        Dict with batched tensors and mask_labels as list\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # --- Fields that need special handling ---\n",
    "    # mask_labels stays as a list because SAM3 expects targets in list format,\n",
    "    # allowing for variable numbers of masks per image during training\n",
    "    list_keys = {\"mask_labels\"}\n",
    "\n",
    "    # --- Stack all standard tensor fields ---\n",
    "    # These include pixel_values, input_ids, attention_mask, etc.\n",
    "    # All samples have the same shape for these, so we can stack them\n",
    "    keys = [k for k in batch[0].keys() if k not in list_keys]\n",
    "    for key in keys:\n",
    "        values = [item[key] for item in batch]\n",
    "        if isinstance(values[0], torch.Tensor):\n",
    "            # Stack tensors along new batch dimension\n",
    "            result[key] = torch.stack(values)\n",
    "        else:\n",
    "            # Non-tensor fields (e.g., strings) stay as lists\n",
    "            result[key] = values\n",
    "\n",
    "    # --- Keep mask_labels as list of tensors ---\n",
    "    # Each element is shape (1, H, W) - one mask per sample\n",
    "    # SAM3's loss function iterates over this list\n",
    "    result[\"mask_labels\"] = [item[\"mask_labels\"] for item in batch]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb2b9b",
   "metadata": {},
   "source": [
    "### Step 3: Split and \"flatten\" the dataset\n",
    "\n",
    "\n",
    "#### Filter by split tags\n",
    "\n",
    "[`match_tags()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.match_tags) returns a view containing only samples with that tag. Your dataset should already have \"train\"/\"val\" tags on each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ced749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples - train: 1486, val: 221\n"
     ]
    }
   ],
   "source": [
    "train_view = dataset.match_tags(\"train\")\n",
    "val_view = dataset.match_tags(\"val\")\n",
    "\n",
    "print(f\"Samples - train: {len(train_view)}, val: {len(val_view)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b54f2",
   "metadata": {},
   "source": [
    "`to_patches(field)` creates a view where each detection in that\n",
    "    field becomes its own sample. If you have 100 images with 5 detections\n",
    "    each, `to_patches` gives you 500 patch samples. This is perfect for \n",
    "    instance-level training like SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe2d9027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches - train: 9712, val: 1396\n"
     ]
    }
   ],
   "source": [
    "train_patches = train_view.to_patches(\"ground_truth\")\n",
    "val_patches = val_view.to_patches(\"ground_truth\")\n",
    "\n",
    "print(f\"Patches - train: {len(train_patches)}, val: {len(val_patches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bbb73",
   "metadata": {},
   "source": [
    "In the patches view, each sample's detection data lives in the original field (e.g., \"ground_truth\"). field_mapping lets us access it with a generic name in our GetItem code.\n",
    "\n",
    "This makes `SAMPatchGetItem` reusable - it always uses `d.get(\"detection\")`, and `field_mapping` tells FiftyOne which actual field that refers to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e915da",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_mapping = {\"detection\": \"ground_truth\"}\n",
    "\n",
    "# we need to instantiate the SAM 3 processor\n",
    "from transformers.models.sam3 import Sam3Processor\n",
    "\n",
    "processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n",
    "\n",
    "train_getter = SAMPatchGetItem(processor, field_mapping=field_mapping)\n",
    "val_getter = SAMPatchGetItem(processor, field_mapping=field_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4881bff",
   "metadata": {},
   "source": [
    "### Step 4: Create DataLoaders\n",
    "\n",
    "`to_torch()` converts a FiftyOne view to a PyTorch Dataset using your `GetItem` class to define how each sample is loaded and transformed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb572f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_patches.to_torch(train_getter)\n",
    "\n",
    "val_dataset = val_patches.to_torch(val_getter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3274f53",
   "metadata": {},
   "source": [
    "\n",
    "Now we can instantiate standard PyTorch DataLoaders with our custom collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f23f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust based on your resources\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bdc1d",
   "metadata": {},
   "source": [
    "Let's take a look at what the data loader yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10f81e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([8, 3, 1008, 1008])\n",
      "original_sizes torch.Size([8, 2])\n",
      "input_ids torch.Size([8, 32])\n",
      "attention_mask torch.Size([8, 32])\n",
      "mask_labels list of 8 items\n",
      "torch.Size([1, 1008, 1008])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k, v.shape)\n",
    "  elif isinstance(v, list):\n",
    "    print(k, f\"list of {len(v)} items\")\n",
    "    if len(v) > 0 and isinstance(v[0], torch.Tensor):\n",
    "      print(v[0].shape)\n",
    "  else:\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f4ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
