{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone umap-learn\n",
    "!pip install git+https://github.com/huggingface/transformers.git#egg=transformers\n",
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c11245",
   "metadata": {},
   "source": [
    "In this tutorial we'll make use of the [RIS-LAD](https://huggingface.co/datasets/Voxel51/RIS-LAD) dataset. [RIS-LAD is the first fine-grained benchmark](https://arxiv.org/abs/2507.20920) designed specifically for low-altitude drone image segmentation.\n",
    "\n",
    "The dataset features 13,871 annotations with image-text-mask triplets captured from real drone footage at 30-100 meter altitudes with oblique viewing angles. Unlike existing remote sensing datasets that rely on high-altitude satellite imagery, RIS-LAD focuses on the visual complexities of low-altitude drone perception. These challenges include perspective changes, densely packed tiny objects, variable lighting conditions, and the notorious problems of **category drift** (tiny targets causing confusion with larger, semantically similar objects) and **object drift** (difficulty distinguishing among crowded same-class instances) that plague crowded aerial scenes.\n",
    "\n",
    "This benchmark addresses the gap in understanding how Visual AI systems see the world from a drone's perspective.\n",
    "\n",
    "You can download the dataset from the Hugging Face Hub as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c562b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/RIS-LAD\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863f43c",
   "metadata": {},
   "source": [
    "This dataset is in [FiftyOne format](https://docs.voxel51.com/user_guide/using_datasets.html). \n",
    "\n",
    "FiftyOne provides powerful functionality to inspect, search, and modify it from a [Dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset)-wide down to a [Sample](https://docs.voxel51.com/api/fiftyone.utils.data.html#fiftyone.utils.data.Sample) level.\n",
    "\n",
    "To see the schema of this dataset, you can simply call the Dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbaefc",
   "metadata": {},
   "source": [
    "A FiftyOne dataset is comprised of [Samples](https://docs.voxel51.com/api/fiftyone.utils.data.html#fiftyone.utils.data.Sample).  \n",
    "\n",
    "Samples store all information associated with a particular piece of data in a dataset, including basic metadata about the data, one or more sets of labels, and additional features associated with subsets of the data and/or label sets.\n",
    "\n",
    "The attributes of a Sample are called [Fields](https://docs.voxel51.com/api/fiftyone.core.fields.html#fiftyone.core.fields.Field), which stores information about the Sample. When a new Field is assigned to a Sample in a Dataset, it is automatically added to the datasetâ€™s schema and thus accessible on all other samples in the dataset.\n",
    "\n",
    "To see the schema of a single Sample and the contents of its Fields, you can call the [`first()` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10948180",
   "metadata": {},
   "source": [
    "You can use the FiftyOne SDK to quickly compute some high-level statistics about your dataset with it's [built-in Aggregration methods](https://docs.voxel51.com/user_guide/using_aggregations.html).\n",
    "\n",
    "For example, you can use the [`count()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count) to compute the number of non-None field values in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f28833",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988745c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count(\"ground_truth.detections.referring_expression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd2b0f",
   "metadata": {},
   "source": [
    "You can use the [`count_values()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count_values) to compute the occurrences of field values in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446256d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32170629",
   "metadata": {},
   "source": [
    "You can use the [`distinct()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.distinct) to compute the distinct values of a field in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b97f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.distinct(\"ground_truth.detections.referring_expression\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342c546",
   "metadata": {},
   "source": [
    "### Adding a new Field to the Dataset\n",
    "\n",
    "A useful piece of information to have about a sample is the number of detection labels in that sample.  You can easily add this to each sample in your Dataset using a `ViewField` expression.  \n",
    "\n",
    "[`ViewField`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewField) and [`ViewExpression`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewExpression) classes allow you to use native Python operators to define expression. Simply wrap the target field of your sample in a `ViewField` and then apply comparison, logic, arithmetic or array operations to it to create a `ViewExpression`\n",
    "\n",
    "The idiomatic FiftyOne way to count the number of instance labels in a sample is to use a `ViewField` expression to access the list of labels and then use `.length()` to count them.\n",
    "\n",
    "To add the number of instances per image as a field on each sample in your dataset, you can use FiftyOne's [`set_values()`](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.set_values) method. This will efficiently compute and store the count for each sample.\n",
    "\n",
    "You can learn more about creating Dataset Views [in these docs](https://docs.voxel51.com/user_guide/using_views.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "num_instances = dataset.values(F(\"ground_truth.detections\").length())\n",
    "\n",
    "dataset.set_values(\"num_instances\", num_instances)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa8df5",
   "metadata": {},
   "source": [
    "In a similar manner, you can count the number of unique instance types for each sample in your Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "labels_per_sample = dataset.values(\"ground_truth.detections.label\")\n",
    "\n",
    "num_distinct_labels_per_sample = [len(set(labels)) if labels else 0 for labels in labels_per_sample]\n",
    "\n",
    "dataset.set_values(\"num_unique_instances\", num_distinct_labels_per_sample)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19f7cd",
   "metadata": {},
   "source": [
    "You can then combine these values together to create a complexity score for each Sample in your Dataset. As a simple example you can define the complexity score as number of instances + number of unique instance types. Note that the [`.values()` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.values) is used for efficiently extracting a slice of field across all Samples in a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_instance_counts = dataset.values(\"num_unique_instances\")\n",
    "\n",
    "num_instances_values = dataset.values(\"num_instances\")\n",
    "\n",
    "# Compute complexity scores for all samples\n",
    "complexity_scores = [nd + nul for nd, nul in zip(num_instances_values, unique_instance_counts)]\n",
    "\n",
    "# Set the values\n",
    "dataset.set_values(\"complexity_score\", complexity_scores)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f14533",
   "metadata": {},
   "source": [
    "There's a lot of interesting and non-trival things, like those shown above, that you can do with Fiftyone. Here are some additional resources for you to check out later:\n",
    "\n",
    "- For those familar with `pandas` you may want to check out this [pandas v FiftyOne cheat sheet](https://docs.voxel51.com/cheat_sheets/pandas_vs_fiftyone.html) to learn how to you can translate common pandas operations into FiftyOne syntax. \n",
    "\n",
    "- How to [create Views of your Dataset](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html) \n",
    "\n",
    "- [Filtering cheat sheet docs](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html)\n",
    "\n",
    "Of course, the most interesting part of FiftyOne is [the FiftyOne App](https://docs.voxel51.com/user_guide/app.html#using-the-fiftyone-app) (which runs locally on your machine). Something that can help us in exploring our Dataset in the App is [the Dashboard plugin](https://docs.voxel51.com/plugins/plugins_ecosystem/dashboard.html). You can install the Plugin as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc0e90",
   "metadata": {},
   "source": [
    "FiftyOne is open-source and hackable, and it has a robust framework for [building Plugins](https://docs.voxel51.com/plugins/developing_plugins.html), which allow you to extend and customize the functionality of the core tool to suit your specific needs.  FiftyOne has integrations with various computer vision models and other popular AI tools, [browse this curated collection of plugins](https://docs.voxel51.com/plugins/) to see how you can transform FiftyOne into a bespoke visual AI development workbench.\n",
    "\n",
    "To launch the FiftyOne App, all you need to do is run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd6da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49871630",
   "metadata": {},
   "source": [
    "<img src=\"ris_lad_in_fo_1.gif\">\n",
    "\n",
    "\n",
    "Of course, you can go deeper in the analysis of your dataset by [visualizing image embeddings](https://docs.voxel51.com/brain.html#visualizing-embeddings) in the App. You can use one of the the models from the [FiftyOne Model Zoo](https://docs.voxel51.com/model_zoo/overview.html), or a custom model which you can integrate as a [Remote Zoo Model](https://docs.voxel51.com/model_zoo/remote.html#remotely-sourced-zoo-models).\n",
    "\n",
    "One example of a Remote Zoo Model is the integration of [SigLIP2](https://docs.voxel51.com/plugins/plugins_ecosystem/siglip2.html), which you can use to visualize image embeddings, perform zero shot classification, and perform image retrieval by [searching via natural language](https://docs.voxel51.com/brain.html#text-similarity) in the App.\n",
    "\n",
    "Let's start by registering the Remote Zoo Model source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register this custom model source\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/siglip2\", \n",
    "    overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2c1ea",
   "metadata": {},
   "source": [
    "Then instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "siglip_model = foz.load_zoo_model(\n",
    "    \"google/siglip2-giant-opt-patch16-256\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ad369",
   "metadata": {},
   "source": [
    "You can than use the [`compute_embeddings()` method](https://docs.voxel51.com/api/fiftyone.core.models.html#fiftyone.core.models.compute_embeddings) of the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80705398",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=siglip_model,\n",
    "    embeddings_field=\"siglip2_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd2ba2",
   "metadata": {},
   "source": [
    "Then use the [`compute_visualization()` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_visualization) to generate low-dimensional representations of the samples (and/or individual objects) in your Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"siglip2_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"siglip2_viz\",\n",
    "    num_dims=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3f6a6",
   "metadata": {},
   "source": [
    "You can then use the [`compute_similarity()` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_similarity) to build a similarity index over the images in your dataset, which allows you to sort by similarity or search with natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a similarity index\n",
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"google/siglip2-giant-opt-patch16-256\",\n",
    "    embeddings=\"siglip2_embeddings\",\n",
    "    brain_key=\"siglip2_similarity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e668e",
   "metadata": {},
   "source": [
    "With the embeddings computed you can perform a lot of non-trival math, such as computing scores for [uniqueness](https://docs.voxel51.com/brain.html#image-uniqueness), [representativeness](https://docs.voxel51.com/brain.html#image-representativeness), and [identifying near duplicates](https://docs.voxel51.com/brain.html#near-duplicates) with simple function calls. \n",
    "\n",
    "\n",
    "We can use the same SigLIP2 model to perform zero-shot classification and further enrich our Dataset with information it didn't have before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip_model.text_prompt = \"Low altitude drone footage taken at \"\n",
    "siglip_model.classes = [\"day\", \"night\", \"dusk\"]\n",
    "\n",
    "dataset.apply_model(\n",
    "    siglip_model,\n",
    "    label_field=\"time_of_day\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d60ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip_model.text_prompt = \"The scene in this low altitude drone footage is in a \"\n",
    "siglip_model.classes = [\"urban area\", \"near water\", \"highway\", \"pedestrian area\"]\n",
    "\n",
    "dataset.apply_model(\n",
    "    siglip_model,\n",
    "    label_field=\"location\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e12bbd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's launch the App again and see what we can uncover by inspecting [the Embeddings panel](https://docs.voxel51.com/user_guide/app.html#embeddings-panel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ac8dfb",
   "metadata": {},
   "source": [
    "<img src=\"ris_lad_in_fo_2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cde1a1",
   "metadata": {},
   "source": [
    "# Using SAM 3\n",
    "\n",
    "We can use [SAM 3 in FiftyOne](https://docs.voxel51.com/plugins/plugins_ecosystem/sam3_images.html) as a Remote Zoo Model. The pattern is exactly as we have seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register the remote model source\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/sam3_images\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "sam3_model = foz.load_zoo_model(\"facebook/sam3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d70d0",
   "metadata": {},
   "source": [
    "The implementation in Fiftyone also allows us to compute embeddings for images using SAM 3 as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "sam3_model.pooling_strategy = \"max\"  # or \"mean\", \"cls\"\n",
    "\n",
    "dataset.compute_embeddings(\n",
    "    sam3_model,\n",
    "    embeddings_field=\"sam_embeddings\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Visualize with UMAP\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    method=\"umap\",\n",
    "    brain_key=\"sam_viz\",\n",
    "    embeddings=\"sam_embeddings\",\n",
    "    num_dims=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87598072",
   "metadata": {},
   "source": [
    "To run the SAM 3 model on the dataset, all we have to do is set some values for the model, and use the [`apply_model()` of the Dataset](docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.dataset.apply_model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam3_model.operation = \"concept_segmentation\"\n",
    "sam3_model.threshold = 0.5\n",
    "sam3_model.mask_threshold = 0.5\n",
    "\n",
    "sam3_model.prompt = dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "dataset.apply_model(\n",
    "    sam3_model,\n",
    "    label_field=\"sam3_not_finetuned\",\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    skip_failures=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa2ecf",
   "metadata": {},
   "source": [
    "We can view the embeddings and the predictions in the App as well:\n",
    "\n",
    "<img src=\"ris_lad_in_fo_3.gif\">\n",
    "\n",
    "We can then use [FiftyOne's evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) to see how well the initial results. You can [`evaluate_detections()` method](https://docs.voxel51.com/user_guide/evaluation.html#detections) to evaluate the predictions of an object detection model stored in a [`Detections`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Detections), [`Polylines`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Polylines), or [`Keypoints`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Keypoints) field of your dataset or of a temporal detection model stored in a [`TemporalDetections`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.TemporalDetection) field of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd15f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_detections(\n",
    "    \"sam3_not_finetuned\",          # Detections with masks\n",
    "    gt_field=\"ground_truth\",   # Detections with masks\n",
    "    eval_key=\"initial_sam3_eval\",\n",
    "    use_masks=True,            # use instance masks for IoU\n",
    "    compute_mAP=True,\n",
    "    tolerance=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a48bb5",
   "metadata": {},
   "source": [
    "The `evaluate_detections()` method returns a [`DetectionResults` instance](https://docs.voxel51.com/api/fiftyone.utils.eval.detection.html#fiftyone.utils.eval.detection.DetectionResults) that provides a variety of methods for generating various aggregate evaluation reports about your model.\n",
    "\n",
    "In addition, when you specify an `eval_key` parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth objects that you can leverage via the FiftyOne App to interactively explore the strengths and weaknesses of your model on individual samples.\n",
    "\n",
    "You can print the report to get a high-level picture of the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff1b500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     bicycle       0.23      0.35      0.28       640\n",
      "        boat       0.32      0.79      0.45       245\n",
      "         bus       0.60      0.71      0.65       732\n",
      "         car       0.34      0.85      0.48      4365\n",
      "       motor       0.23      0.80      0.35      2803\n",
      "      people       0.20      0.50      0.29      2910\n",
      "    tricycle       0.54      0.43      0.48       528\n",
      "       truck       0.51      0.49      0.50      1648\n",
      "\n",
      "   micro avg       0.29      0.68      0.40     13871\n",
      "   macro avg       0.37      0.62      0.43     13871\n",
      "weighted avg       0.32      0.68      0.42     13871\n",
      "\n",
      "0.27819947196402245\n"
     ]
    }
   ],
   "source": [
    "results.print_report()\n",
    "print(results.mAP())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebafbd",
   "metadata": {},
   "source": [
    "You can also open the [Model Evaluation Panel](https://docs.voxel51.com/api/fiftyone.utils.eval.detection.html#fiftyone.utils.eval.detection.DetectionResults) to visualize and interactively explore the evaluation results in the App:\n",
    "\n",
    "<img src=\"ris_lad_in_fo_4.gif\">\n",
    "\n",
    "\n",
    "You can use [Scenario Analysis](https://docs.voxel51.com/user_guide/app.html#scenario-analysis-sub-new) for a deep dive into model behavior across different scenarios.\n",
    "\n",
    "This evaluation technique helps uncover edge cases, identify annotation errors, and understand performance variations in different contexts. It gives you a better insight into your model's strengths and weaknesses while enabling meaningful comparisons of performance under varying input conditions. \n",
    "\n",
    "Ultimately, this detailed analysis helps improve training data quality and builds intuition about when and why your model succeeds or fails.\n",
    "\n",
    "<img src=\"ris_lad_in_fo_5.gif\">\n",
    "\n",
    "#### We're almost ready to fine-tune the model, but before we do we should check if there is any data leakage between the train and validation sets of the dataset.\n",
    "\n",
    "Our dataset has [Sample level tags](https://docs.voxel51.com/user_guide/basics.html#tags) which indicate which split each sample belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6bd115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train', 'val']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.distinct(\"tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d2e20",
   "metadata": {},
   "source": [
    "Despite our best efforts, duplicates and other forms of non-IID samples show up in our data. \n",
    "\n",
    "When these samples end up in different splits, [this can have consequences when evaluating a model](https://voxel51.com/blog/on-leaky-datasets-and-a-clever-horse). It can often be easy to overestimate model capability due to this issue. The FiftyOne Brain offers a way to identify such cases in dataset splits.\n",
    "\n",
    "The leaks of a dataset can be computed directly without the need for the predictions of a pre-trained model via the [`compute_leaky_splits()`](https://docs.voxel51.com/brain.html#leaky-splits) method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e048ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        Voxel51/RIS-LAD\n",
       "Media type:  image\n",
       "Num samples: 2103\n",
       "Persistent:  True\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:                   fiftyone.core.fields.ObjectIdField\n",
       "    filepath:             fiftyone.core.fields.StringField\n",
       "    tags:                 fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:             fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:           fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at:     fiftyone.core.fields.DateTimeField\n",
       "    ground_truth:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    prompts:              fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    siglip2_embeddings:   fiftyone.core.fields.VectorField\n",
       "    time_of_day:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    location:             fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    sam_embeddings:       fiftyone.core.fields.VectorField\n",
       "    sam3_not_finetuned:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    initial_sam3_eval_tp: fiftyone.core.fields.IntField\n",
       "    initial_sam3_eval_fp: fiftyone.core.fields.IntField\n",
       "    initial_sam3_eval_fn: fiftyone.core.fields.IntField"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing duplicate samples...\n",
      "Duplicates computation complete\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "split_tags = [\"train\", \"val\"]\n",
    "\n",
    "index = fob.compute_leaky_splits(\n",
    "    dataset, \n",
    "    splits=split_tags,\n",
    "    embeddings=\"sam_embeddings\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e42157",
   "metadata": {},
   "source": [
    "The [`leaks_view()` method](https://docs.voxel51.com/api/fiftyone.brain.internal.core.leaky_splits.html#fiftyone.brain.internal.core.leaky_splits.LeakySplitsIndex.leaks_view) returns a view that contains only the leaks in the input splits. Once you have these leaks, it is wise to look through them. You may gain some insight into the source of the leaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ee55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks = index.leaks_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f26d23",
   "metadata": {},
   "source": [
    "You can launch the app on this view like so:\n",
    "\n",
    "```python\n",
    "session = fo.launch_app(leaks)\n",
    "```\n",
    "\n",
    "Fortunately for us, there are no leaks between our splits. But, it's always a good idea to check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
