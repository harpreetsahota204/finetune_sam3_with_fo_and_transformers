{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone umap-learn\n",
    "!pip install git+https://github.com/huggingface/transformers.git#egg=transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c11245",
   "metadata": {},
   "source": [
    "In this tutorial we'll make use of the [RIS-LAD](https://huggingface.co/datasets/Voxel51/RIS-LAD) dataset. [RIS-LAD is the first fine-grained benchmark](https://arxiv.org/abs/2507.20920) designed specifically for low-altitude drone image segmentation.\n",
    "\n",
    "The dataset features 13,871 annotations with image-text-mask triplets captured from real drone footage at 30-100 meter altitudes with oblique viewing angles. Unlike existing remote sensing datasets that rely on high-altitude satellite imagery, RIS-LAD focuses on the visual complexities of low-altitude drone perception. These challenges include perspective changes, densely packed tiny objects, variable lighting conditions, and the notorious problems of **category drift** (tiny targets causing confusion with larger, semantically similar objects) and **object drift** (difficulty distinguishing among crowded same-class instances) that plague crowded aerial scenes.\n",
    "\n",
    "This benchmark addresses the gap in understanding how Visual AI systems see the world from a drone's perspective.\n",
    "\n",
    "You can download the dataset from the Hugging Face Hub as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c562b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/RIS-LAD\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863f43c",
   "metadata": {},
   "source": [
    "This dataset is in [FiftyOne format](https://docs.voxel51.com/user_guide/using_datasets.html). \n",
    "\n",
    "FiftyOne provides powerful functionality to inspect, search, and modify it from a [Dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset)-wide down to a [Sample](https://docs.voxel51.com/api/fiftyone.utils.data.html#fiftyone.utils.data.Sample) level.\n",
    "\n",
    "To see the schema of this dataset, you can simply call the Dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbaefc",
   "metadata": {},
   "source": [
    "A FiftyOne dataset is comprised of [Samples](https://docs.voxel51.com/api/fiftyone.utils.data.html#fiftyone.utils.data.Sample).  \n",
    "\n",
    "Samples store all information associated with a particular piece of data in a dataset, including basic metadata about the data, one or more sets of labels, and additional features associated with subsets of the data and/or label sets.\n",
    "\n",
    "The attributes of a Sample are called [Fields](https://docs.voxel51.com/api/fiftyone.core.fields.html#fiftyone.core.fields.Field), which stores information about the Sample. When a new Field is assigned to a Sample in a Dataset, it is automatically added to the dataset’s schema and thus accessible on all other samples in the dataset.\n",
    "\n",
    "To see the schema of a single Sample and the contents of its Fields, you can call the [`first()` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10948180",
   "metadata": {},
   "source": [
    "You can use the FiftyOne SDK to quickly compute some high-level statistics about your dataset with it's [built-in Aggregration methods](https://docs.voxel51.com/user_guide/using_aggregations.html).\n",
    "\n",
    "For example, you can use the [`count()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count) to compute the number of non-None field values in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f28833",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988745c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count(\"ground_truth.detections.referring_expression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd2b0f",
   "metadata": {},
   "source": [
    "You can use the [`count_values()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count_values) to compute the occurrences of field values in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446256d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32170629",
   "metadata": {},
   "source": [
    "You can use the [`distinct()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.distinct) to compute the distinct values of a field in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b97f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.distinct(\"ground_truth.detections.referring_expression\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342c546",
   "metadata": {},
   "source": [
    "### Adding a new Field to the Dataset\n",
    "\n",
    "A useful piece of information to have about a sample is the number of detection labels in that sample.  You can easily add this to each sample in your Dataset using a `ViewField` expression.  \n",
    "\n",
    "[`ViewField`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewField) and [`ViewExpression`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewExpression) classes allow you to use native Python operators to define expression. Simply wrap the target field of your sample in a `ViewField` and then apply comparison, logic, arithmetic or array operations to it to create a `ViewExpression`\n",
    "\n",
    "The idiomatic FiftyOne way to count the number of instance labels in a sample is to use a `ViewField` expression to access the list of labels and then use `.length()` to count them.\n",
    "\n",
    "To add the number of instances per image as a field on each sample in your dataset, you can use FiftyOne's [`set_values()`](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.set_values) method. This will efficiently compute and store the count for each sample.\n",
    "\n",
    "You can learn more about creating Dataset Views [in these docs](https://docs.voxel51.com/user_guide/using_views.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "num_instances = dataset.values(F(\"ground_truth.detections\").length())\n",
    "\n",
    "dataset.set_values(\"num_instances\", num_instances)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa8df5",
   "metadata": {},
   "source": [
    "In a similar manner, you can count the number of unique instance types for each sample in your Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "labels_per_sample = dataset.values(\"ground_truth.detections.label\")\n",
    "\n",
    "num_distinct_labels_per_sample = [len(set(labels)) if labels else 0 for labels in labels_per_sample]\n",
    "\n",
    "dataset.set_values(\"num_unique_instances\", num_distinct_labels_per_sample)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19f7cd",
   "metadata": {},
   "source": [
    "You can then combine these values together to create a complexity score for each Sample in your Dataset. As a simple example you can define the complexity score as number of instances + number of unique instance types. Note that the [`.values()` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.values) is used for efficiently extracting a slice of field across all Samples in a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_instance_counts = dataset.values(\"num_unique_instances\")\n",
    "\n",
    "num_instances_values = dataset.values(\"num_instances\")\n",
    "\n",
    "# Compute complexity scores for all samples\n",
    "complexity_scores = [nd + nul for nd, nul in zip(num_instances_values, unique_instance_counts)]\n",
    "\n",
    "# Set the values\n",
    "dataset.set_values(\"complexity_score\", complexity_scores)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f14533",
   "metadata": {},
   "source": [
    "There's a lot of interesting and non-trival things. like those shown above, that you can do with Fiftyone. Here are some additional resources for you to check out later:\n",
    "\n",
    "- For those familar with `pandas` you may want to check out this [pandas v FiftyOne cheat sheet](https://docs.voxel51.com/cheat_sheets/pandas_vs_fiftyone.html) to learn how to you can translate common pandas operations into FiftyOne syntax. \n",
    "\n",
    "- How to [create Views of your Dataset](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html) \n",
    "\n",
    "- [Filtering cheat sheet docs](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html)\n",
    "\n",
    "Of course, the most interesting part of FiftyOne is [the FiftyOne App](https://docs.voxel51.com/user_guide/app.html#using-the-fiftyone-app) (which runs locally on your machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd598407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49871630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register this custom model source\n",
    "foz.register_zoo_model_source(\"https://github.com/harpreetsahota204/siglip2\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "siglip_model = foz.load_zoo_model(\n",
    "    \"google/siglip2-giant-opt-patch16-256\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80705398",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=siglip_model,\n",
    "    embeddings_field=\"siglip2_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"siglip2_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"siglip2_viz\",\n",
    "    num_dims=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a similarity index\n",
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"google/siglip2-giant-opt-patch16-256\",\n",
    "    embeddings=\"siglip2_embeddings\",\n",
    "    brain_key=\"siglip2_similarity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip_model.text_prompt = \"Low altitude drone footage taken at \"\n",
    "siglip_model.classes = [\"day\", \"night\", \"dusk\"]\n",
    "\n",
    "dataset.apply_model(\n",
    "    siglip_model,\n",
    "    label_field=\"time_of_day\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d60ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip_model.text_prompt = \"The scene in this low altitude drone footage is in a \"\n",
    "siglip_model.classes = [\"urban area\", \"near water\", \"highway\", \"pedestrian area\"]\n",
    "\n",
    "dataset.apply_model(\n",
    "    siglip_model,\n",
    "    label_field=\"location\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0239c26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/harpreetsahota204/sam3_images...\n",
      "  594.8Mb [3.9s elapsed, ? remaining, 616.3Mb/s] \n",
      "Overwriting existing model source '/home/harpreet/fiftyone/__models__/sam3'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6634b24c9d6b498e92b5a2ff28dd6e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486b228835e8410bb46f283974dc988d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de512341947481b84c76eb6cde95774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register the remote model source\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/sam3_images\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "sam3_model = foz.load_zoo_model(\"facebook/sam3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "sam3_model.pooling_strategy = \"max\"  # or \"mean\", \"cls\"\n",
    "\n",
    "dataset.compute_embeddings(\n",
    "    sam3_model,\n",
    "    embeddings_field=\"sam_embeddings\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Visualize with UMAP\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    method=\"umap\",\n",
    "    brain_key=\"sam_viz\",\n",
    "    embeddings=\"sam_embeddings\",\n",
    "    num_dims=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36be60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  37% |█████\\---------|  768/2103 [38.2m elapsed, 1.1h remaining, 0.3 samples/s] "
     ]
    }
   ],
   "source": [
    "sam3_model.operation = \"concept_segmentation\"\n",
    "sam3_model.threshold = 0.5\n",
    "sam3_model.mask_threshold = 0.5\n",
    "\n",
    "sam3_model.prompt = dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "dataset.apply_model(\n",
    "    sam3_model,\n",
    "    label_field=\"sam3_not_finetuned\",\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    skip_failures=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd15f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_detections(\n",
    "    \"sam3_not_finetuned\",          # Detections with masks\n",
    "    gt_field=\"ground_truth\",   # Detections with masks\n",
    "    eval_key=\"initial_sam3_eval\",\n",
    "    use_masks=True,            # use instance masks for IoU\n",
    "    compute_mAP=True,\n",
    ")\n",
    "\n",
    "results.print_report()\n",
    "print(results.mAP())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
