{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125c98d1",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/finetune_sam3_with_fo_and_transformers/blob/main/how_to_finetune_sam3.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone umap-learn\n",
    "!pip install git+https://github.com/huggingface/transformers.git#egg=transformers\n",
    "!pip install shapely\n",
    "!pip install trackio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c11245",
   "metadata": {},
   "source": [
    "In this tutorial we'll make use of the [RIS-LAD](https://huggingface.co/datasets/Voxel51/RIS-LAD) dataset. [RIS-LAD is the first fine-grained benchmark](https://arxiv.org/abs/2507.20920) designed specifically for low-altitude drone image segmentation.\n",
    "\n",
    "The dataset features 13,871 annotations with image-text-mask triplets captured from real drone footage at 30-100 meter altitudes with oblique viewing angles. Unlike existing remote sensing datasets that rely on high-altitude satellite imagery, RIS-LAD focuses on the visual complexities of low-altitude drone perception. These challenges include perspective changes, densely packed tiny objects, variable lighting conditions, and the notorious problems of **category drift** (tiny targets causing confusion with larger, semantically similar objects) and **object drift** (difficulty distinguishing among crowded same-class instances) that plague crowded aerial scenes.\n",
    "\n",
    "# Download the Dataset\n",
    "\n",
    "This benchmark addresses the gap in understanding how Visual AI systems see the world from a drone's perspective.\n",
    "\n",
    "You can download the dataset from the Hugging Face Hub as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c562b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/RIS-LAD\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863f43c",
   "metadata": {},
   "source": [
    "# Explore the Dataset\n",
    "\n",
    "This dataset is in [FiftyOne format](https://docs.voxel51.com/user_guide/using_datasets.html). \n",
    "\n",
    "FiftyOne provides powerful functionality to inspect, search, and modify it from a [Dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset)-wide down to a [Sample](https://docs.voxel51.com/api/fiftyone.utils.data.html#fiftyone.utils.data.Sample) level.\n",
    "\n",
    "To see the schema of this dataset, you can simply call the Dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbaefc",
   "metadata": {},
   "source": [
    "A FiftyOne dataset is comprised of [Samples](https://docs.voxel51.com/api/fiftyone.utils.data.html#fiftyone.utils.data.Sample).  \n",
    "\n",
    "Samples store all information associated with a particular piece of data in a dataset, including basic metadata about the data, one or more sets of labels, and additional features associated with subsets of the data and/or label sets.\n",
    "\n",
    "The attributes of a Sample are called [Fields](https://docs.voxel51.com/api/fiftyone.core.fields.html#fiftyone.core.fields.Field), which stores information about the Sample. When a new Field is assigned to a Sample in a Dataset, it is automatically added to the dataset’s schema and thus accessible on all other samples in the dataset.\n",
    "\n",
    "To see the schema of a single Sample and the contents of its Fields, you can call the [`first()` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10948180",
   "metadata": {},
   "source": [
    "You can use the FiftyOne SDK to quickly compute some high-level statistics about your dataset with it's [built-in Aggregration methods](https://docs.voxel51.com/user_guide/using_aggregations.html).\n",
    "\n",
    "For example, you can use the [`count()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count) to compute the number of non-None field values in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f28833",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988745c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count(\"ground_truth.detections.referring_expression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd2b0f",
   "metadata": {},
   "source": [
    "You can use the [`count_values()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count_values) to compute the occurrences of field values in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446256d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count_values(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32170629",
   "metadata": {},
   "source": [
    "You can use the [`distinct()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.distinct) to compute the distinct values of a field in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b97f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.distinct(\"ground_truth.detections.referring_expression\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342c546",
   "metadata": {},
   "source": [
    "### Adding a new Field to the Dataset\n",
    "\n",
    "A useful piece of information to have about a sample is the number of detection labels in that sample.  You can easily add this to each sample in your Dataset using a `ViewField` expression.  \n",
    "\n",
    "[`ViewField`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewField) and [`ViewExpression`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewExpression) classes allow you to use native Python operators to define expression. Simply wrap the target field of your sample in a `ViewField` and then apply comparison, logic, arithmetic or array operations to it to create a `ViewExpression`\n",
    "\n",
    "The idiomatic FiftyOne way to count the number of instance labels in a sample is to use a `ViewField` expression to access the list of labels and then use `.length()` to count them.\n",
    "\n",
    "To add the number of instances per image as a field on each sample in your dataset, you can use FiftyOne's [`set_values()`](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.set_values) method. This will efficiently compute and store the count for each sample.\n",
    "\n",
    "You can learn more about creating Dataset Views [in these docs](https://docs.voxel51.com/user_guide/using_views.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "num_instances = dataset.values(F(\"ground_truth.detections\").length())\n",
    "\n",
    "dataset.set_values(\"num_instances\", num_instances)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa8df5",
   "metadata": {},
   "source": [
    "In a similar manner, you can count the number of unique instance types for each sample in your Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "labels_per_sample = dataset.values(\"ground_truth.detections.label\")\n",
    "\n",
    "num_distinct_labels_per_sample = [len(set(labels)) if labels else 0 for labels in labels_per_sample]\n",
    "\n",
    "dataset.set_values(\"num_unique_instances\", num_distinct_labels_per_sample)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19f7cd",
   "metadata": {},
   "source": [
    "You can then combine these values together to create a complexity score for each Sample in your Dataset. As a simple example you can define the complexity score as number of instances + number of unique instance types. Note that the [`.values()` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.values) is used for efficiently extracting a slice of field across all Samples in a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_instance_counts = dataset.values(\"num_unique_instances\")\n",
    "\n",
    "num_instances_values = dataset.values(\"num_instances\")\n",
    "\n",
    "# Compute complexity scores for all samples\n",
    "complexity_scores = [nd + nul for nd, nul in zip(num_instances_values, unique_instance_counts)]\n",
    "\n",
    "# Set the values\n",
    "dataset.set_values(\"complexity_score\", complexity_scores)\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f14533",
   "metadata": {},
   "source": [
    "There's a lot of interesting and non-trival things, like those shown above, that you can do with Fiftyone. Here are some additional resources for you to check out later:\n",
    "\n",
    "- For those familar with `pandas` you may want to check out this [pandas v FiftyOne cheat sheet](https://docs.voxel51.com/cheat_sheets/pandas_vs_fiftyone.html) to learn how to you can translate common pandas operations into FiftyOne syntax. \n",
    "\n",
    "- How to [create Views of your Dataset](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html) \n",
    "\n",
    "- [Filtering cheat sheet docs](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html)\n",
    "\n",
    "Of course, the most interesting part of FiftyOne is [the FiftyOne App](https://docs.voxel51.com/user_guide/app.html#using-the-fiftyone-app) (which runs locally on your machine). Something that can help us in exploring our Dataset in the App is [the Dashboard plugin](https://docs.voxel51.com/plugins/plugins_ecosystem/dashboard.html). You can install the Plugin as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc0e90",
   "metadata": {},
   "source": [
    "FiftyOne is open-source and hackable, and it has a robust framework for [building Plugins](https://docs.voxel51.com/plugins/developing_plugins.html), which allow you to extend and customize the functionality of the core tool to suit your specific needs.  FiftyOne has integrations with various computer vision models and other popular AI tools, [browse this curated collection of plugins](https://docs.voxel51.com/plugins/) to see how you can transform FiftyOne into a bespoke visual AI development workbench.\n",
    "\n",
    "To launch the FiftyOne App, all you need to do is run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd6da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49871630",
   "metadata": {},
   "source": [
    "<img src=\"ris_lad_in_fo_1.gif\">\n",
    "\n",
    "\n",
    "Of course, you can go deeper in the analysis of your dataset by [visualizing image embeddings](https://docs.voxel51.com/brain.html#visualizing-embeddings) in the App. You can use one of the the models from the [FiftyOne Model Zoo](https://docs.voxel51.com/model_zoo/overview.html), or a custom model which you can integrate as a [Remote Zoo Model](https://docs.voxel51.com/model_zoo/remote.html#remotely-sourced-zoo-models).\n",
    "\n",
    "One example of a Remote Zoo Model is the integration of [SigLIP2](https://docs.voxel51.com/plugins/plugins_ecosystem/siglip2.html), which you can use to visualize image embeddings, perform zero shot classification, and perform image retrieval by [searching via natural language](https://docs.voxel51.com/brain.html#text-similarity) in the App.\n",
    "\n",
    "Let's start by registering the Remote Zoo Model source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register this custom model source\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/siglip2\", \n",
    "    overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2c1ea",
   "metadata": {},
   "source": [
    "Then instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "siglip_model = foz.load_zoo_model(\n",
    "    \"google/siglip2-giant-opt-patch16-256\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ad369",
   "metadata": {},
   "source": [
    "You can than use the [`compute_embeddings()` method](https://docs.voxel51.com/api/fiftyone.core.models.html#fiftyone.core.models.compute_embeddings) of the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80705398",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=siglip_model,\n",
    "    embeddings_field=\"siglip2_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd2ba2",
   "metadata": {},
   "source": [
    "Then use the [`compute_visualization()` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_visualization) to generate low-dimensional representations of the samples (and/or individual objects) in your Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"siglip2_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"siglip2_viz\",\n",
    "    num_dims=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3f6a6",
   "metadata": {},
   "source": [
    "You can then use the [`compute_similarity()` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_similarity) to build a similarity index over the images in your dataset, which allows you to sort by similarity or search with natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a similarity index\n",
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"google/siglip2-giant-opt-patch16-256\",\n",
    "    embeddings=\"siglip2_embeddings\",\n",
    "    brain_key=\"siglip2_similarity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e668e",
   "metadata": {},
   "source": [
    "With the embeddings computed you can perform a lot of non-trival math, such as computing scores for [uniqueness](https://docs.voxel51.com/brain.html#image-uniqueness), [representativeness](https://docs.voxel51.com/brain.html#image-representativeness), and [identifying near duplicates](https://docs.voxel51.com/brain.html#near-duplicates) with simple function calls. \n",
    "\n",
    "\n",
    "We can use the same SigLIP2 model to perform zero-shot classification and further enrich our Dataset with information it didn't have before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip_model.text_prompt = \"Low altitude drone footage taken at \"\n",
    "siglip_model.classes = [\"day\", \"night\", \"dusk\"]\n",
    "\n",
    "dataset.apply_model(\n",
    "    siglip_model,\n",
    "    label_field=\"time_of_day\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d60ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "siglip_model.text_prompt = \"The scene in this low altitude drone footage is in a \"\n",
    "siglip_model.classes = [\"urban area\", \"near water\", \"highway\", \"pedestrian area\"]\n",
    "\n",
    "dataset.apply_model(\n",
    "    siglip_model,\n",
    "    label_field=\"location\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e12bbd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's launch the App again and see what we can uncover by inspecting [the Embeddings panel](https://docs.voxel51.com/user_guide/app.html#embeddings-panel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ac8dfb",
   "metadata": {},
   "source": [
    "<img src=\"ris_lad_in_fo_2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cde1a1",
   "metadata": {},
   "source": [
    "# SAM 3 Initial Results (Before Fine-Tuning)\n",
    "\n",
    "We can use [SAM 3 in FiftyOne](https://docs.voxel51.com/plugins/plugins_ecosystem/sam3_images.html) as a Remote Zoo Model. The pattern is exactly as we have seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register the remote model source\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/sam3_images\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "sam3_model = foz.load_zoo_model(\"facebook/sam3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d70d0",
   "metadata": {},
   "source": [
    "The implementation in Fiftyone also allows us to compute embeddings for images using SAM 3 as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "sam3_model.pooling_strategy = \"max\"  # or \"mean\", \"cls\"\n",
    "\n",
    "dataset.compute_embeddings(\n",
    "    sam3_model,\n",
    "    embeddings_field=\"sam_embeddings\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Visualize with UMAP\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    method=\"umap\",\n",
    "    brain_key=\"sam_viz\",\n",
    "    embeddings=\"sam_embeddings\",\n",
    "    num_dims=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87598072",
   "metadata": {},
   "source": [
    "To run the SAM 3 model on the dataset, all we have to do is set some values for the model, and use the [`apply_model()` of the Dataset](docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.dataset.apply_model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam3_model.operation = \"concept_segmentation\"\n",
    "sam3_model.threshold = 0.5\n",
    "sam3_model.mask_threshold = 0.5\n",
    "\n",
    "sam3_model.prompt = dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "dataset.apply_model(\n",
    "    sam3_model,\n",
    "    label_field=\"sam3_not_finetuned\",\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    skip_failures=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa2ecf",
   "metadata": {},
   "source": [
    "We can view the embeddings and the predictions in the App as well:\n",
    "\n",
    "<img src=\"ris_lad_in_fo_3.gif\">\n",
    "\n",
    "We can then use [FiftyOne's evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) to see how well the initial results. You can [`evaluate_detections()` method](https://docs.voxel51.com/user_guide/evaluation.html#detections) to evaluate the predictions of an object detection model stored in a [`Detections`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Detections), [`Polylines`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Polylines), or [`Keypoints`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Keypoints) field of your dataset or of a temporal detection model stored in a [`TemporalDetections`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.TemporalDetection) field of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd15f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_detections(\n",
    "    \"sam3_not_finetuned\",          # Detections with masks\n",
    "    gt_field=\"ground_truth\",   # Detections with masks\n",
    "    eval_key=\"initial_sam3_eval\",\n",
    "    use_masks=True,            # use instance masks for IoU\n",
    "    compute_mAP=True,\n",
    "    tolerance=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a48bb5",
   "metadata": {},
   "source": [
    "The `evaluate_detections()` method returns a [`DetectionResults` instance](https://docs.voxel51.com/api/fiftyone.utils.eval.detection.html#fiftyone.utils.eval.detection.DetectionResults) that provides a variety of methods for generating various aggregate evaluation reports about your model.\n",
    "\n",
    "In addition, when you specify an `eval_key` parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth objects that you can leverage via the FiftyOne App to interactively explore the strengths and weaknesses of your model on individual samples.\n",
    "\n",
    "You can print the report to get a high-level picture of the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.print_report()\n",
    "print(results.mAP())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebafbd",
   "metadata": {},
   "source": [
    "You can also open the [Model Evaluation Panel](https://docs.voxel51.com/api/fiftyone.utils.eval.detection.html#fiftyone.utils.eval.detection.DetectionResults) to visualize and interactively explore the evaluation results in the App:\n",
    "\n",
    "<img src=\"ris_lad_in_fo_4.gif\">\n",
    "\n",
    "\n",
    "You can use [Scenario Analysis](https://docs.voxel51.com/user_guide/app.html#scenario-analysis-sub-new) for a deep dive into model behavior across different scenarios.\n",
    "\n",
    "This evaluation technique helps uncover edge cases, identify annotation errors, and understand performance variations in different contexts. It gives you a better insight into your model's strengths and weaknesses while enabling meaningful comparisons of performance under varying input conditions. \n",
    "\n",
    "Ultimately, this detailed analysis helps improve training data quality and builds intuition about when and why your model succeeds or fails.\n",
    "\n",
    "<img src=\"ris_lad_in_fo_5.gif\">\n",
    "\n",
    "#### Check for Data Leakage between Train and Validation Splits\n",
    "\n",
    "We're almost ready to fine-tune the model, but before we do we should check if there is any data leakage between the train and validation sets of the dataset.\n",
    "\n",
    "Our dataset has [Sample level tags](https://docs.voxel51.com/user_guide/basics.html#tags) which indicate which split each sample belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6bd115",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.distinct(\"tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d2e20",
   "metadata": {},
   "source": [
    "Despite our best efforts, duplicates and other forms of non-IID samples show up in our data. \n",
    "\n",
    "When these samples end up in different splits, [this can have consequences when evaluating a model](https://voxel51.com/blog/on-leaky-datasets-and-a-clever-horse). It can often be easy to overestimate model capability due to this issue. The FiftyOne Brain offers a way to identify such cases in dataset splits.\n",
    "\n",
    "The leaks of a dataset can be computed directly without the need for the predictions of a pre-trained model via the [`compute_leaky_splits()`](https://docs.voxel51.com/brain.html#leaky-splits) method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "split_tags = [\"train\", \"val\"]\n",
    "\n",
    "index = fob.compute_leaky_splits(\n",
    "    dataset, \n",
    "    splits=split_tags,\n",
    "    embeddings=\"sam_embeddings\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e42157",
   "metadata": {},
   "source": [
    "The [`leaks_view()` method](https://docs.voxel51.com/api/fiftyone.brain.internal.core.leaky_splits.html#fiftyone.brain.internal.core.leaky_splits.LeakySplitsIndex.leaks_view) returns a view that contains only the leaks in the input splits. Once you have these leaks, it is wise to look through them. You may gain some insight into the source of the leaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks = index.leaks_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f26d23",
   "metadata": {},
   "source": [
    "You can launch the app on this view like so:\n",
    "\n",
    "```python\n",
    "session = fo.launch_app(leaks)\n",
    "```\n",
    "\n",
    "Fortunately for us, there are no leaks between our splits. But, it's always a good idea to check.\n",
    "\n",
    "We're now ready to fine-tune SAM 3\n",
    "\n",
    "# SAM 3 Fine-tuning, Part 1: PyTorch Dataset from FiftyOne Dataset\n",
    "\n",
    "First, we need to convert the FiftyOne Dataset to a torch Dataset.\n",
    "\n",
    "In this section we will convert a FiftyOne dataset with Detection masks into a PyTorch \n",
    "dataset compatible with SAM fine-tuning.\n",
    "\n",
    "Key insight: FiftyOne's [`to_patches()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_patches) method creates a view where each \n",
    "detection becomes its own sample. This eliminates the need to manually \n",
    "flatten detections - FiftyOne handles it for us.\n",
    "\n",
    "The pipeline:\n",
    "1. Define a [`GetItem`](https://docs.voxel51.com/api/fiftyone.utils.torch.html#fiftyone.utils.torch.GetItem) to extract and transform each patch\n",
    "\n",
    "2. Define a Collate Function\n",
    "\n",
    "3. Split and flatten the dataset by converting dataset to patches view (one sample per detection)\n",
    "\n",
    "4. Use [`to_torch()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_torch) to create the DataLoader PyTorch dataset\n",
    "\n",
    "\n",
    "### Step 1: Define a GetItem subclass\n",
    "\n",
    "FiftyOne's `GetItem` class is the bridge between FiftyOne and PyTorch. It tells \n",
    "FiftyOne:\n",
    "\n",
    " 1. What fields to extract from each sample (via `required_keys`)\n",
    " \n",
    " 2. How to transform them into your desired format (via `__call__`)\n",
    "\n",
    "The `field_mapping` parameter is important when working with patches. In a \n",
    "patches view, the detection data lives in the original field name (e.g., \n",
    "\"ground_truth\"), but we want to access it with a generic name in our code.\n",
    "\n",
    "`field_mapping={\"detection\": \"ground_truth\"}` means:\n",
    " - In our code, we write `d[\"detection\"]` \n",
    " - FiftyOne knows to pull from the \"ground_truth\" field\n",
    "\n",
    "This makes our `GetItem` reusable across datasets with different field names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fiftyone.utils.torch import GetItem\n",
    "\n",
    "class SAMPatchGetItem(GetItem):\n",
    "    \"\"\"\n",
    "    Extracts and transforms patch data for SAM training.\n",
    "    \n",
    "    Each patch sample contains:\n",
    "    - filepath: path to the full image\n",
    "    - detection: the Detection object (bbox, mask, label, etc.)\n",
    "    - metadata: image dimensions\n",
    "    \n",
    "    We transform this into SAM's expected format:\n",
    "    - pixel_values: processed image tensor\n",
    "    - mask_labels: ground truth mask resized to match model input\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processor, field_mapping=None):\n",
    "        self.processor = processor\n",
    "        # Must call super().__init__() with field_mapping - this sets up\n",
    "        # the internal mapping that FiftyOne uses to pull the right fields\n",
    "        super().__init__(field_mapping=field_mapping)\n",
    "\n",
    "    @property\n",
    "    def required_keys(self):\n",
    "        # These are the keys we'll access in __call__.\n",
    "        # 'detection' is a virtual key that gets mapped to the real field\n",
    "        # via field_mapping. 'filepath' and 'metadata' are standard fields\n",
    "        # that exist on all FiftyOne samples.\n",
    "        return [\"filepath\", \"detection\", \"metadata\"]\n",
    "\n",
    "    def __call__(self, d):\n",
    "        \"\"\"\n",
    "        Transform a FiftyOne sample dict into SAM training format.\n",
    "        \n",
    "        This is where the FiftyOne → SAM conversion happens:\n",
    "        - Cropped mask → full-image mask → resized to model input size\n",
    "        - Raw image + text label → processed tensors\n",
    "        \"\"\"\n",
    "        # Load full image (patches still reference the original image file)\n",
    "        image = Image.open(d[\"filepath\"]).convert(\"RGB\")\n",
    "        detection = d[\"detection\"]\n",
    "        metadata = d[\"metadata\"]\n",
    "\n",
    "        # Get image dimensions (cast to int for safety)\n",
    "        w = int(metadata.width)\n",
    "        h = int(metadata.height)\n",
    "\n",
    "        # --- Bounding Box Extraction ---\n",
    "        # FiftyOne stores bboxes as [x, y, width, height] with values in [0, 1].\n",
    "        # We only need the top-left corner (x0, y0) to position the mask.\n",
    "        rx, ry, rw, rh = detection.bounding_box\n",
    "        x0 = int(rx * w)  # Top-left x in pixels\n",
    "        y0 = int(ry * h)  # Top-left y in pixels\n",
    "\n",
    "        # --- Mask Conversion ---\n",
    "        # FiftyOne stores masks cropped to the bounding box to save space.\n",
    "        # We expand the cropped mask back to full image size by placing it\n",
    "        # at the correct position (x0, y0).\n",
    "        full_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        m = detection.mask\n",
    "        mh, mw = m.shape\n",
    "        full_mask[y0 : y0 + mh, x0 : x0 + mw] = m.astype(np.uint8)\n",
    "\n",
    "        # --- Text Prompt ---\n",
    "        # Use the detection's class label as the text prompt for SAM\n",
    "        text = detection[\"label\"]\n",
    "\n",
    "        # --- SAM Processor ---\n",
    "        # The processor handles image preprocessing and text tokenization.\n",
    "        # For text-prompted SAM variants, we pass both image and text.\n",
    "        inputs = self.processor(images=image, text=text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Remove batch dimension added by processor (we batch later in DataLoader)\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "        # --- Resize Mask to Match Model Input ---\n",
    "        # The processor resizes the image to the model's expected input size.\n",
    "        # We need to resize our ground truth mask to match, so the loss\n",
    "        # computation compares tensors of the same shape.\n",
    "        hm = int(inputs[\"pixel_values\"].shape[-2])  # Model input height\n",
    "        wm = int(inputs[\"pixel_values\"].shape[-1])  # Model input width\n",
    "        \n",
    "        # Convert mask to tensor and add batch+channel dims for interpolate\n",
    "        mask_t = torch.from_numpy(full_mask).float()[None, None, ...]  # (1, 1, H, W)\n",
    "        \n",
    "        # Resize using nearest neighbor to preserve binary mask values\n",
    "        mask_rs = F.interpolate(mask_t, size=(hm, wm), mode=\"nearest\").squeeze(0)  # (1, H, W)\n",
    "\n",
    "        # Add resized mask as the training label\n",
    "        inputs[\"mask_labels\"] = mask_rs\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82215597",
   "metadata": {},
   "source": [
    "### Step 2: Collate function for DataLoader\n",
    "\n",
    "When PyTorch's DataLoader batches samples together, it needs to know how to combine them. \n",
    "\n",
    "The default collate works for simple tensors, but we have:\n",
    "\n",
    " - `mask_labels`: must stay as a list because SAM3 expects variable-length \n",
    "   targets per image (even though we have one mask per sample here, the model\n",
    "   interface expects a list)\n",
    "\n",
    "A custom collate function tells DataLoader exactly how to handle each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for SAM3 training.\n",
    "\n",
    "    Handles batching of samples from SAMPatchGetItem:\n",
    "    - Stacks tensor fields (pixel_values, input_ids, attention_mask, etc.)\n",
    "    - Keeps mask_labels as a list of tensors (SAM3's expected format)\n",
    "    \n",
    "    Args:\n",
    "        batch: List of dicts from SAMPatchGetItem.__call__\n",
    "        \n",
    "    Returns:\n",
    "        Dict with batched tensors and mask_labels as list\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # --- Fields that need special handling ---\n",
    "    # mask_labels stays as a list because SAM3 expects targets in list format,\n",
    "    # allowing for variable numbers of masks per image during training\n",
    "    list_keys = {\"mask_labels\"}\n",
    "\n",
    "    # --- Stack all standard tensor fields ---\n",
    "    # These include pixel_values, input_ids, attention_mask, etc.\n",
    "    # All samples have the same shape for these, so we can stack them\n",
    "    keys = [k for k in batch[0].keys() if k not in list_keys]\n",
    "    for key in keys:\n",
    "        values = [item[key] for item in batch]\n",
    "        if isinstance(values[0], torch.Tensor):\n",
    "            # Stack tensors along new batch dimension\n",
    "            result[key] = torch.stack(values)\n",
    "        else:\n",
    "            # Non-tensor fields (e.g., strings) stay as lists\n",
    "            result[key] = values\n",
    "\n",
    "    # --- Keep mask_labels as list of tensors ---\n",
    "    # Each element is shape (1, H, W) - one mask per sample\n",
    "    # SAM3's loss function iterates over this list\n",
    "    result[\"mask_labels\"] = [item[\"mask_labels\"] for item in batch]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb2b9b",
   "metadata": {},
   "source": [
    "### Step 3: Split and \"Flatten\" the Dataset\n",
    "\n",
    "\n",
    "#### Filter by split tags\n",
    "\n",
    "[`match_tags()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.match_tags) returns a view containing only samples with that tag. Your dataset should already have \"train\"/\"val\" tags on each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ced749",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_view = dataset.match_tags(\"train\")\n",
    "val_view = dataset.match_tags(\"val\")\n",
    "\n",
    "print(f\"Samples - train: {len(train_view)}, val: {len(val_view)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b54f2",
   "metadata": {},
   "source": [
    "`to_patches(field)` creates a view where each detection in that\n",
    "    field becomes its own sample. If you have 100 images with 5 detections\n",
    "    each, `to_patches` gives you 500 patch samples. This is perfect for \n",
    "    instance-level training like SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches = train_view.to_patches(\"ground_truth\")\n",
    "val_patches = val_view.to_patches(\"ground_truth\")\n",
    "\n",
    "print(f\"Patches - train: {len(train_patches)}, val: {len(val_patches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bbb73",
   "metadata": {},
   "source": [
    "In the patches view, each sample's detection data lives in the original field (e.g., \"ground_truth\"). field_mapping lets us access it with a generic name in our GetItem code.\n",
    "\n",
    "This makes `SAMPatchGetItem` reusable - it always uses `d.get(\"detection\")`, and `field_mapping` tells FiftyOne which actual field that refers to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e915da",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_mapping = {\"detection\": \"ground_truth\"}\n",
    "\n",
    "# we need to instantiate the SAM 3 processor\n",
    "from transformers.models.sam3 import Sam3Processor\n",
    "\n",
    "processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n",
    "\n",
    "train_getter = SAMPatchGetItem(processor, field_mapping=field_mapping)\n",
    "val_getter = SAMPatchGetItem(processor, field_mapping=field_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4881bff",
   "metadata": {},
   "source": [
    "### Step 4: Create DataLoaders\n",
    "\n",
    "`to_torch()` converts a FiftyOne view to a PyTorch Dataset using your `GetItem` class to define how each sample is loaded and transformed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb572f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_patches.to_torch(train_getter)\n",
    "\n",
    "val_dataset = val_patches.to_torch(val_getter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3274f53",
   "metadata": {},
   "source": [
    "\n",
    "Now we can instantiate standard PyTorch DataLoaders with our custom collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust based on your resources\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bdc1d",
   "metadata": {},
   "source": [
    "Let's take a look at what the data loader yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f81e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k, v.shape)\n",
    "  elif isinstance(v, list):\n",
    "    print(k, f\"list of {len(v)} items\")\n",
    "    if len(v) > 0 and isinstance(v[0], torch.Tensor):\n",
    "      print(v[0].shape)\n",
    "  else:\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949e5ed",
   "metadata": {},
   "source": [
    "# SAM 3 Fine-tuning, Part 2: Optimizer, Loss Function, and Training Loop\n",
    "\n",
    "Let's define the learning rate, number of training epochs, and instantiate the model. Note that we have already instantiated the processor before, but for completeness it is instantiated again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "num_epochs = 1\n",
    "log_every = 200\n",
    "\n",
    "from transformers.models.sam3 import Sam3Model, Sam3Processor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n",
    "model = Sam3Model.from_pretrained(\"facebook/sam3\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee00b4f",
   "metadata": {},
   "source": [
    "We can now define the optimizer and the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8bc97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884bcdc3",
   "metadata": {},
   "source": [
    "### We'll implement a simple Hungarian matcher and loss for SAM 3. \n",
    "\n",
    "The [**Hungarian algorithm** (also called the Kuhn-Munkres algorithm)](https://cp-algorithms.com/graph/hungarian-algorithm.html) solves the **optimal assignment problem**: given N workers and M jobs with different costs for each worker-job pairing, find the assignment that minimizes total cost.\n",
    "\n",
    "##### In the Context of Segmentation\n",
    "\n",
    "When SAM 3 predicts **Q masks** but you have **T ground truth masks**, you need to figure out *which prediction corresponds to which target* before you can compute loss. This is the matching problem.\n",
    "\n",
    "**What the matcher does:**\n",
    "\n",
    "1. **Computes a cost matrix** — For every (prediction, target) pair, it calculates `1 - IoU` (so lower cost = better overlap)\n",
    "\n",
    "2. **Finds optimal 1-to-1 assignment** — `linear_sum_assignment` finds the matching that minimizes total cost across all pairs\n",
    "\n",
    "3. **Returns indices** — `(row_ind, col_ind)` tells you prediction `i` matches target `j`\n",
    "\n",
    "##### Why We Do This\n",
    "\n",
    "Without matching, you'd have an ambiguity problem. \n",
    "\n",
    "If SAM 3 outputs 5 masks and you have 3 ground truth objects, which predictions do you penalize?\n",
    "\n",
    "Random assignment would create noisy gradients. The Hungarian matcher ensures each prediction is compared against its *best-fitting* target, giving meaningful supervision.\n",
    "\n",
    "##### The Loss Pipeline\n",
    "\n",
    "```\n",
    "Predictions (Q masks)  ──┐\n",
    "                         ├──► Hungarian Match ──► Paired masks \n",
    "                         |──► Dice + BCE Loss\n",
    "Targets (T masks)      ──┘\n",
    "```\n",
    "\n",
    "This pattern (Hungarian matching + set-based loss) was popularized by **DETR** and is standard in transformer-based detection/segmentation where you have unordered set predictions rather than anchor-based outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddba984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def hungarian_matcher(pred_masks, target_masks):\n",
    "    \"\"\"Match predictions to targets using Hungarian assignment on mask IoU.\n",
    "\n",
    "    pred_masks: [Q, Hp, Wp] logits\n",
    "    target_masks: [T, Ht, Wt] binary/float\n",
    "\n",
    "    SAM3 often outputs masks at a lower internal resolution than `pixel_values`.\n",
    "    We resize targets to (Hp, Wp) before matching.\n",
    "    \"\"\"\n",
    "    # Edge case: no ground truth masks to match against\n",
    "    if target_masks.shape[0] == 0:\n",
    "        return (\n",
    "            torch.tensor([], dtype=torch.int64, device=pred_masks.device),\n",
    "            torch.tensor([], dtype=torch.int64, device=pred_masks.device),\n",
    "        )\n",
    "\n",
    "    # Resize targets to match prediction resolution (SAM3 often outputs at lower res)\n",
    "    if pred_masks.shape[-2:] != target_masks.shape[-2:]:\n",
    "        tgt = target_masks[:, None, ...].float()  # [T,1,Ht,Wt] - add channel dim for interpolate\n",
    "        tgt = nn.functional.interpolate(tgt, size=pred_masks.shape[-2:], mode=\"nearest\")\n",
    "        target_masks = tgt[:, 0, ...]  # [T,Hp,Wp] - remove channel dim\n",
    "\n",
    "    # Convert to probabilities and flatten spatial dims for efficient matrix ops\n",
    "    pred_sigmoid = pred_masks.sigmoid().flatten(1)  # [Q, Hp*Wp] - Q predictions flattened\n",
    "    target_flat = target_masks.float().flatten(1)   # [T, Hp*Wp] - T targets flattened\n",
    "\n",
    "    # ---- Compute IoU between all (prediction, target) pairs ----\n",
    "    # Matrix multiply gives us sum(pred * target) for each pair = intersection\n",
    "    intersection = torch.matmul(pred_sigmoid, target_flat.T)  # [Q, T]\n",
    "    pred_area = pred_sigmoid.sum(1, keepdim=True)             # [Q, 1] - area of each pred\n",
    "    target_area = target_flat.sum(1, keepdim=True).T          # [1, T] - area of each target\n",
    "    # Union = A + B - intersection (broadcasting gives [Q, T] matrix)\n",
    "    union = (pred_area + target_area - intersection).clamp_min(1e-6)\n",
    "\n",
    "    # IoU matrix: entry [i,j] = IoU between prediction i and target j\n",
    "    iou = intersection / union\n",
    "    # Cost = 1 - IoU (lower cost = better match, Hungarian minimizes cost)\n",
    "    cost = (1 - iou).detach().cpu().numpy()\n",
    "\n",
    "    # ---- Hungarian algorithm finds optimal 1-to-1 assignment ----\n",
    "    # Returns indices: prediction row_ind[k] matches target col_ind[k]\n",
    "    row_ind, col_ind = linear_sum_assignment(cost)\n",
    "    return (\n",
    "        torch.as_tensor(row_ind, dtype=torch.int64, device=pred_masks.device),\n",
    "        torch.as_tensor(col_ind, dtype=torch.int64, device=pred_masks.device),\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_loss(pred_masks, target_masks):\n",
    "    \"\"\"Dice + BCE on matched masks.\n",
    "\n",
    "    pred_masks: [N, Hp, Wp] logits (N = number of matched pairs)\n",
    "    target_masks: [N, Ht, Wt] binary {0,1}\n",
    "    \"\"\"\n",
    "    # No predictions to supervise - return zero loss with grad enabled\n",
    "    if pred_masks.numel() == 0:\n",
    "        return torch.tensor(0.0, device=pred_masks.device, requires_grad=True)\n",
    "\n",
    "    # Align target resolution to prediction resolution if needed\n",
    "    if pred_masks.shape[-2:] != target_masks.shape[-2:]:\n",
    "        tgt = target_masks[:, None, ...].float()  # [N,1,Ht,Wt]\n",
    "        tgt = nn.functional.interpolate(tgt, size=pred_masks.shape[-2:], mode=\"nearest\")\n",
    "        target_masks = tgt[:, 0, ...]\n",
    "\n",
    "    # Flatten spatial dimensions: each mask becomes a 1D vector\n",
    "    pred_flat = pred_masks.flatten(1)              # [N, Hp*Wp] - raw logits\n",
    "    target_flat = target_masks.float().flatten(1)  # [N, Hp*Wp] - binary targets\n",
    "\n",
    "    # ---- Dice Loss ----\n",
    "    # Measures overlap; works well for imbalanced masks (few fg pixels)\n",
    "    pred_sigmoid = pred_flat.sigmoid()  # Convert logits to probabilities\n",
    "    # Dice = 2 * |A ∩ B| / (|A| + |B|), so loss = 1 - Dice\n",
    "    numerator = 2 * (pred_sigmoid * target_flat).sum(1)   # 2 * intersection per mask\n",
    "    denominator = pred_sigmoid.sum(1) + target_flat.sum(1)  # sum of areas\n",
    "    dice_loss = 1 - (numerator + 1) / (denominator + 1)  # +1 for numerical stability\n",
    "    dice_loss = dice_loss.mean()  # Average over all matched pairs\n",
    "\n",
    "    # ---- BCE Loss ----\n",
    "    # Per-pixel classification loss; complements Dice with pixel-level gradients\n",
    "    bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "        pred_flat, target_flat, reduction=\"mean\"\n",
    "    )\n",
    "\n",
    "    # Combined loss: Dice handles global overlap, BCE handles pixel accuracy\n",
    "    return dice_loss + bce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f4264",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n",
    "\n",
    "This function measures how well the model is performing on held-out data **without updating weights**. For each batch:\n",
    "\n",
    "1. **Forward pass** — Run images + text prompts through SAM3 to get predicted masks\n",
    "\n",
    "2. **Match predictions to ground truth** — Use Hungarian matching to pair each prediction with its best-matching target\n",
    "\n",
    "3. **Compute loss on matched pairs** — Calculate Dice + BCE loss to quantify prediction quality\n",
    "\n",
    "4. **Accumulate** — Average the loss across all samples to get a single validation metric\n",
    "\n",
    "The `@torch.no_grad()` decorator disables gradient computation since we're only measuring performance, not training. This saves memory and speeds things up.\n",
    "\n",
    "Evaluation mirrors training logic (forward pass → match → loss) but skips the backward pass. This gives you a comparable metric to your training loss, letting you detect overfitting (training loss drops but val loss plateaus/rises)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0305b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(dataloader, max_batches=20):\n",
    "    model.eval()\n",
    "\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for b_idx, batch in enumerate(dataloader):\n",
    "        if max_batches is not None and b_idx >= max_batches:\n",
    "            break\n",
    "\n",
    "        outputs = model(\n",
    "            pixel_values=batch[\"pixel_values\"].to(device),\n",
    "            input_ids=batch[\"input_ids\"].to(device),\n",
    "            attention_mask=batch.get(\"attention_mask\", None).to(device) if batch.get(\"attention_mask\", None) is not None else None,\n",
    "        )\n",
    "\n",
    "        predicted_masks = outputs.pred_masks  # [B,Q,H,W]\n",
    "        mask_labels = [m.to(device) for m in batch[\"mask_labels\"]]  # list([T,H,W])\n",
    "\n",
    "        for i in range(len(mask_labels)):\n",
    "            src_idx, tgt_idx = hungarian_matcher(predicted_masks[i], mask_labels[i])\n",
    "            if len(src_idx) > 0:\n",
    "                loss = compute_loss(predicted_masks[i][src_idx], mask_labels[i][tgt_idx])\n",
    "                total += float(loss.item())\n",
    "                n += 1\n",
    "\n",
    "    model.train()\n",
    "    return (total / max(n, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbad9dc",
   "metadata": {},
   "source": [
    "We need to visualize our predictions during training so we can monitor the model improvement. We'll create a function to show evaluation masks using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def show_mask(mask, ax, color=None, alpha=0.6):\n",
    "    \"\"\"Overlay a binary mask on a matplotlib axis.\"\"\"\n",
    "    if color is None:\n",
    "        color = np.array([0.12, 0.56, 1.0])\n",
    "    mask = mask.detach().float().cpu().numpy() if isinstance(mask, torch.Tensor) else np.asarray(mask)\n",
    "    if mask.ndim == 3:\n",
    "        mask = mask[0]\n",
    "    h, w = mask.shape[-2:]\n",
    "    rgba = np.concatenate([np.asarray(color).reshape(3), [alpha]])\n",
    "    ax.imshow(mask.reshape(h, w, 1) * rgba.reshape(1, 1, 4))\n",
    "\n",
    "\n",
    "def denorm_pixel_values(pixel_values, processor):\n",
    "    \"\"\"Denormalize for visualization.\"\"\"\n",
    "    x = pixel_values.detach().float().cpu()\n",
    "    if x.ndim == 4:\n",
    "        x = x[0]\n",
    "\n",
    "    mean = None\n",
    "    std = None\n",
    "    ip = getattr(processor, \"image_processor\", None)\n",
    "    if ip is not None:\n",
    "        print(\"ip is not none\")\n",
    "        mean = getattr(ip, \"image_mean\", None)\n",
    "        std = getattr(ip, \"image_std\", None)\n",
    "        print(\"mean and std\", ip)\n",
    "\n",
    "    if mean is not None and std is not None:\n",
    "        mean = torch.tensor(mean).view(3, 1, 1)\n",
    "        std = torch.tensor(std).view(3, 1, 1)\n",
    "        x = x * std + mean\n",
    "\n",
    "    x = x.clamp(0, 1)\n",
    "    return x.permute(1, 2, 0).numpy()\n",
    "\n",
    "\n",
    "def _mask_iou(pred_bin: torch.Tensor, tgt_bin: torch.Tensor) -> torch.Tensor:\n",
    "    pred = pred_bin.bool()\n",
    "    tgt = tgt_bin.bool()\n",
    "    inter = (pred & tgt).sum().float()\n",
    "    union = (pred | tgt).sum().float().clamp_min(1.0)\n",
    "    return inter / union\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_batch(batch, step, processor, model, max_items=2, thresh=0.5, title_prefix=\"\"):\n",
    "    \"\"\"Visualize input + GT + best predicted mask.\n",
    "\n",
    "    We use raw `outputs.pred_masks` (mask queries) to keep a clean 1-to-1 matching\n",
    "    story (query -> target). `processor.post_process_instance_segmentation(...)`\n",
    "    is great for inference-style results, but it can filter/merge/re-rank masks,\n",
    "    which breaks straightforward query-wise matching.\n",
    "\n",
    "    SAM3 `pred_masks` are typically lower-res than `pixel_values`, so we:\n",
    "    - downsample GT for IoU selection\n",
    "    - upsample the chosen prediction for overlay\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    outputs = model(\n",
    "        pixel_values=batch[\"pixel_values\"].to(device),\n",
    "        input_ids=batch[\"input_ids\"].to(device),\n",
    "        attention_mask=batch.get(\"attention_mask\", None).to(device) if batch.get(\"attention_mask\", None) is not None else None,\n",
    "    )\n",
    "    pred_masks = outputs.pred_masks\n",
    "    bsz = pred_masks.shape[0]\n",
    "    n = min(max_items, bsz)\n",
    "    fig, axes = plt.subplots(n, 3, figsize=(12, 4 * n))\n",
    "    if n == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i in range(n):\n",
    "        pv = batch[\"pixel_values\"][i]\n",
    "        img = denorm_pixel_values(pv, processor)\n",
    "        hm, wm = pv.shape[-2], pv.shape[-1]\n",
    "\n",
    "        tgt = batch[\"mask_labels\"][i].to(pred_masks.device)\n",
    "        tgt1 = tgt[0]\n",
    "\n",
    "        pm = pred_masks[i]\n",
    "        hp, wp = pm.shape[-2], pm.shape[-1]\n",
    "\n",
    "        # Downsample GT to pred resolution for fair IoU\n",
    "        tgt_ds = F.interpolate(tgt1[None, None, ...].float(), size=(hp, wp), mode=\"nearest\")[0, 0]\n",
    "\n",
    "        best_j = 0\n",
    "        best_iou = -1.0\n",
    "        for j in range(pm.shape[0]):\n",
    "            iou = _mask_iou(pm[j].sigmoid() > thresh, tgt_ds > 0.5).item()\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_j = j\n",
    "\n",
    "        # Upsample chosen pred to pixel_values resolution for overlay\n",
    "        pred_up = F.interpolate(pm[best_j][None, None, ...].sigmoid(), size=(hm, wm), mode=\"bilinear\", align_corners=False)[0, 0]\n",
    "        pred_bin = (pred_up > thresh).float().detach().cpu()\n",
    "\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title(\"Input\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(img)\n",
    "        show_mask(tgt1.detach().cpu(), axes[i, 1], color=np.array([0.0, 1.0, 0.0]))\n",
    "        axes[i, 1].set_title(\"GT\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        axes[i, 2].imshow(img)\n",
    "        show_mask(pred_bin, axes[i, 2], color=np.array([1.0, 0.0, 0.0]))\n",
    "        axes[i, 2].set_title(f\"Pred (best IoU={best_iou:.3f})\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"{title_prefix}Step {step}\".strip())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c751e0b",
   "metadata": {},
   "source": [
    "We can now write our training loop and train, while tracking our experiments via [`trackio`](https://huggingface.co/docs/trackio/en/index)\n",
    "\n",
    "`trackio` is a lightweight, free experiment tracking Python library built on top of Hugging Face Datasets and Spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cee474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import trackio\n",
    "\n",
    "trackio.init(project=\"sam3-rislad\")\n",
    "\n",
    "model.train()\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26d541",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "Each epoch processes the full training set, and each batch update follows the standard deep learning recipe:\n",
    "\n",
    "1. **Forward pass** — Feed images and text prompts through SAM3 to get mask predictions\n",
    "\n",
    "2. **Hungarian matching** — For each image, optimally pair predictions with ground truth masks\n",
    "\n",
    "3. **Loss computation** — Calculate Dice + BCE loss on matched pairs, averaged across images in the batch\n",
    "\n",
    "4. **Backward pass** — Compute gradients of the loss with respect to model parameters\n",
    "\n",
    "5. **Optimizer step** — Update weights in the direction that reduces loss\n",
    "\n",
    "The loop also includes periodic validation (every `log_every` steps) to monitor generalization, plus visualization to qualitatively inspect predictions.A few subtle points worth noting:\n",
    "\n",
    "- **`total_loss + loss_i`** (not `+=`) — This keeps the computation graph intact for backprop. Using `+=` on a Python float would detach gradients.\n",
    "\n",
    "- **`set_to_none=True`** — Slightly more efficient than zeroing gradients; sets `.grad` to `None` instead of filling with zeros.\n",
    "\n",
    "- **Visualizing both train and val** — Lets you visually compare whether the model generalizes or just memorizes training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []  # Track losses for this epoch (for logging/averaging)\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"epoch {epoch}\"):\n",
    "        # ---- Forward pass: generate mask predictions ----\n",
    "        outputs = model(\n",
    "            pixel_values=batch[\"pixel_values\"].to(device),      # Images [B, C, H, W]\n",
    "            input_ids=batch[\"input_ids\"].to(device),            # Tokenized text prompts\n",
    "            attention_mask=batch.get(\"attention_mask\", None).to(device) \n",
    "                if batch.get(\"attention_mask\", None) is not None else None,\n",
    "        )\n",
    "\n",
    "        predicted_masks = outputs.pred_masks  # [B, Q, H, W] - Q candidate masks per image\n",
    "        mask_labels = [m.to(device) for m in batch[\"mask_labels\"]]  # List of [T, H, W] targets\n",
    "\n",
    "        # ---- Compute loss for each image in the batch ----\n",
    "        total_loss = 0.0\n",
    "        num_images = 0\n",
    "        for i in range(len(mask_labels)):\n",
    "            # Find optimal 1-to-1 assignment between predictions and targets\n",
    "            src_idx, tgt_idx = hungarian_matcher(predicted_masks[i], mask_labels[i])\n",
    "            \n",
    "            if len(src_idx) > 0:  # Only compute loss if we have valid matches\n",
    "                # Index matched pairs and compute Dice + BCE loss\n",
    "                loss_i = compute_loss(predicted_masks[i][src_idx], mask_labels[i][tgt_idx])\n",
    "                total_loss = total_loss + loss_i  # Accumulate (keeps gradient graph intact)\n",
    "                num_images += 1\n",
    "\n",
    "        # Average loss across images in batch (avoid div-by-zero)\n",
    "        loss = total_loss / max(num_images, 1)\n",
    "\n",
    "        # ---- Backward pass + weight update ----\n",
    "        optimizer.zero_grad(set_to_none=True)  # Clear old gradients (set_to_none is slightly faster)\n",
    "        loss.backward()                         # Compute gradients via backpropagation\n",
    "        optimizer.step()                        # Update model weights\n",
    "\n",
    "        # ---- Logging ----\n",
    "        epoch_losses.append(float(loss.item()))\n",
    "        step += 1\n",
    "\n",
    "        # Optional: log to trackio if enabled\n",
    "        if globals().get(\"_trackio_enabled\", False):\n",
    "            trackio.log({\"train/loss\": float(loss.item()), \"epoch\": int(epoch), \"step\": int(step)})\n",
    "\n",
    "        # ---- Periodic validation + visualization ----\n",
    "        if step % log_every == 0:\n",
    "            # Run evaluation on validation set (limited batches for speed)\n",
    "            eval_loss = evaluate(val_dataloader, max_batches=10)\n",
    "            print(f\"step {step} | train_loss={mean(epoch_losses[-log_every:]):.4f} | val_loss={eval_loss:.4f}\")\n",
    "\n",
    "            if globals().get(\"_trackio_enabled\", False):\n",
    "                trackio.log({\"val/loss\": float(eval_loss), \"epoch\": int(epoch), \"step\": int(step)})\n",
    "\n",
    "            # Visualize predictions on training and validation samples\n",
    "            # Helps catch issues like mode collapse, poor localization, etc.\n",
    "            visualize_batch(batch, step, processor, model, max_items=2, title_prefix=\"[train] \")\n",
    "            viz_batch = next(iter(val_dataloader))\n",
    "            visualize_batch(viz_batch, step, processor, model, max_items=2, title_prefix=\"[val] \")\n",
    "\n",
    "    print(f\"Epoch {epoch} done | mean train loss: {mean(epoch_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69aee98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc2fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
